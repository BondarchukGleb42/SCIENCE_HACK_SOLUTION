{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "398851bc",
   "metadata": {},
   "source": [
    "# Решение конкурсного задания участника Бондарчук Глеб (GlebBondarchuk42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da6d977",
   "metadata": {},
   "source": [
    "В этой тетрадке описано моё решение конкурсного задания. Для решения используется очень много ML моделей, точность которых при обучении может меняться из-за по-разному сгенерированных распределений весов. Веса моделей с самой высокой точностью были сохранены в pkl файлы, приложенные к ноутбуку. При самостоятельном обучении точность моделей может меняться с разбросом ~ 2-6%, а также занимать много времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb5910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import umap.umap_ as umap\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "pd.options.display.max_columns = 90\n",
    "pd.options.display.max_rows = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f618d5",
   "metadata": {},
   "source": [
    "#### Загрузим и посмотрим на наши данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0163a065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data//For_model_labled.xlsx\")\n",
    "test_df = pd.read_excel(\"data//For_check_unlabled.xlsx\")\n",
    "\n",
    "events = test_df[\"Event\"]\n",
    "df.drop([\"Run\", \"Event\"], axis=1, inplace=True)\n",
    "df = df.dropna()\n",
    "test_df.drop([\"Run\", \"Event\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bcc9d772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>px1</th>\n",
       "      <th>py1</th>\n",
       "      <th>pz1</th>\n",
       "      <th>pt1</th>\n",
       "      <th>eta1</th>\n",
       "      <th>phi1</th>\n",
       "      <th>Q1</th>\n",
       "      <th>E2</th>\n",
       "      <th>px2</th>\n",
       "      <th>py2</th>\n",
       "      <th>pz2</th>\n",
       "      <th>pt2</th>\n",
       "      <th>eta2</th>\n",
       "      <th>phi2</th>\n",
       "      <th>M</th>\n",
       "      <th>Q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.085</td>\n",
       "      <td>-4.591</td>\n",
       "      <td>20.111</td>\n",
       "      <td>12.433</td>\n",
       "      <td>20.628</td>\n",
       "      <td>0.571</td>\n",
       "      <td>1.795</td>\n",
       "      <td>-1</td>\n",
       "      <td>2.572</td>\n",
       "      <td>-2.157</td>\n",
       "      <td>-1.308</td>\n",
       "      <td>0.501</td>\n",
       "      <td>2.523</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-2.596</td>\n",
       "      <td>12.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.946</td>\n",
       "      <td>-4.678</td>\n",
       "      <td>5.188</td>\n",
       "      <td>-21.857</td>\n",
       "      <td>6.985</td>\n",
       "      <td>-1.858</td>\n",
       "      <td>2.304</td>\n",
       "      <td>1</td>\n",
       "      <td>67.161</td>\n",
       "      <td>-8.248</td>\n",
       "      <td>11.371</td>\n",
       "      <td>-65.676</td>\n",
       "      <td>14.047</td>\n",
       "      <td>-2.247</td>\n",
       "      <td>2.198</td>\n",
       "      <td>4.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.393</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-1.935</td>\n",
       "      <td>1.365</td>\n",
       "      <td>1.966</td>\n",
       "      <td>0.648</td>\n",
       "      <td>-1.394</td>\n",
       "      <td>1</td>\n",
       "      <td>33.862</td>\n",
       "      <td>-30.874</td>\n",
       "      <td>-8.121</td>\n",
       "      <td>11.291</td>\n",
       "      <td>31.924</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-2.884</td>\n",
       "      <td>11.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80.323</td>\n",
       "      <td>-14.436</td>\n",
       "      <td>26.110</td>\n",
       "      <td>74.576</td>\n",
       "      <td>29.835</td>\n",
       "      <td>1.647</td>\n",
       "      <td>2.076</td>\n",
       "      <td>1</td>\n",
       "      <td>22.382</td>\n",
       "      <td>2.309</td>\n",
       "      <td>7.145</td>\n",
       "      <td>21.085</td>\n",
       "      <td>7.509</td>\n",
       "      <td>1.756</td>\n",
       "      <td>1.258</td>\n",
       "      <td>12.01</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.209</td>\n",
       "      <td>2.846</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-1.439</td>\n",
       "      <td>2.868</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>1</td>\n",
       "      <td>27.966</td>\n",
       "      <td>-4.059</td>\n",
       "      <td>-9.865</td>\n",
       "      <td>-25.851</td>\n",
       "      <td>10.668</td>\n",
       "      <td>-1.618</td>\n",
       "      <td>-1.961</td>\n",
       "      <td>11.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       E1     px1     py1     pz1     pt1   eta1   phi1  Q1      E2     px2  \\\n",
       "0  24.085  -4.591  20.111  12.433  20.628  0.571  1.795  -1   2.572  -2.157   \n",
       "1  22.946  -4.678   5.188 -21.857   6.985 -1.858  2.304   1  67.161  -8.248   \n",
       "2   2.393   0.346  -1.935   1.365   1.966  0.648 -1.394   1  33.862 -30.874   \n",
       "3  80.323 -14.436  26.110  74.576  29.835  1.647  2.076   1  22.382   2.309   \n",
       "4   3.209   2.846  -0.352  -1.439   2.868 -0.483 -0.123   1  27.966  -4.059   \n",
       "\n",
       "      py2     pz2     pt2   eta2   phi2      M  Q2  \n",
       "0  -1.308   0.501   2.523  0.197 -2.596  12.01   1  \n",
       "1  11.371 -65.676  14.047 -2.247  2.198   4.01   1  \n",
       "2  -8.121  11.291  31.924  0.347 -2.884  11.01   1  \n",
       "3   7.145  21.085   7.509  1.756  1.258  12.01  -1  \n",
       "4  -9.865 -25.851  10.668 -1.618 -1.961  11.01   1  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b3712e",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0fcb1f",
   "metadata": {},
   "source": [
    "Сразу взглянем на матрицу корреляций наших данных. Можно заметить, что целевая переменная Q2 практически не коррелирует ни с какими другими признаками, что может вызвать серьёзные затруднения при работе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fd2caaec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DjGle\\AppData\\Local\\Temp\\ipykernel_15444\\4022063383.py:1: FutureWarning: this method is deprecated in favour of `Styler.format(precision=..)`\n",
      "  df.corr().style.background_gradient(cmap='coolwarm').set_precision(3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a4f2c_row0_col0, #T_a4f2c_row1_col1, #T_a4f2c_row2_col2, #T_a4f2c_row3_col3, #T_a4f2c_row4_col4, #T_a4f2c_row5_col5, #T_a4f2c_row6_col6, #T_a4f2c_row7_col7, #T_a4f2c_row8_col8, #T_a4f2c_row9_col9, #T_a4f2c_row10_col10, #T_a4f2c_row11_col11, #T_a4f2c_row12_col12, #T_a4f2c_row13_col13, #T_a4f2c_row14_col14, #T_a4f2c_row15_col15, #T_a4f2c_row16_col16 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row0_col1, #T_a4f2c_row1_col10, #T_a4f2c_row7_col10, #T_a4f2c_row8_col2, #T_a4f2c_row9_col2, #T_a4f2c_row11_col2, #T_a4f2c_row12_col2, #T_a4f2c_row13_col2, #T_a4f2c_row16_col1, #T_a4f2c_row16_col2 {\n",
       "  background-color: #90b2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row0_col2, #T_a4f2c_row3_col8 {\n",
       "  background-color: #98b9ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row0_col3, #T_a4f2c_row1_col9, #T_a4f2c_row2_col10, #T_a4f2c_row2_col14, #T_a4f2c_row3_col0, #T_a4f2c_row7_col16, #T_a4f2c_row8_col0, #T_a4f2c_row8_col4, #T_a4f2c_row8_col5, #T_a4f2c_row8_col11, #T_a4f2c_row8_col13, #T_a4f2c_row9_col1, #T_a4f2c_row10_col2, #T_a4f2c_row10_col6, #T_a4f2c_row11_col8, #T_a4f2c_row11_col12, #T_a4f2c_row11_col15, #T_a4f2c_row16_col7 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row0_col4, #T_a4f2c_row4_col0 {\n",
       "  background-color: #f59c7d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row0_col5, #T_a4f2c_row12_col0 {\n",
       "  background-color: #3d50c3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row0_col6, #T_a4f2c_row0_col13, #T_a4f2c_row15_col5 {\n",
       "  background-color: #7a9df8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row0_col7, #T_a4f2c_row2_col0, #T_a4f2c_row4_col7, #T_a4f2c_row16_col12 {\n",
       "  background-color: #6a8bef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row0_col8 {\n",
       "  background-color: #84a7fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row0_col9, #T_a4f2c_row3_col1, #T_a4f2c_row4_col9, #T_a4f2c_row4_col13, #T_a4f2c_row5_col1, #T_a4f2c_row12_col9, #T_a4f2c_row16_col9 {\n",
       "  background-color: #8badfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row0_col10, #T_a4f2c_row4_col10, #T_a4f2c_row13_col10 {\n",
       "  background-color: #89acfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row0_col11, #T_a4f2c_row3_col13 {\n",
       "  background-color: #b2ccfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row0_col12 {\n",
       "  background-color: #445acc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row0_col14, #T_a4f2c_row1_col13, #T_a4f2c_row4_col6, #T_a4f2c_row4_col14, #T_a4f2c_row5_col15, #T_a4f2c_row7_col6, #T_a4f2c_row9_col6, #T_a4f2c_row12_col13, #T_a4f2c_row12_col14, #T_a4f2c_row13_col6, #T_a4f2c_row15_col14, #T_a4f2c_row16_col13 {\n",
       "  background-color: #7597f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row0_col15 {\n",
       "  background-color: #aec9fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row0_col16, #T_a4f2c_row2_col7, #T_a4f2c_row3_col7, #T_a4f2c_row3_col16, #T_a4f2c_row4_col16, #T_a4f2c_row5_col7, #T_a4f2c_row6_col16, #T_a4f2c_row7_col0, #T_a4f2c_row7_col4, #T_a4f2c_row7_col12, #T_a4f2c_row9_col7, #T_a4f2c_row10_col16, #T_a4f2c_row11_col16, #T_a4f2c_row13_col0, #T_a4f2c_row13_col16, #T_a4f2c_row14_col7 {\n",
       "  background-color: #6485ec;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row1_col0, #T_a4f2c_row5_col4, #T_a4f2c_row8_col7, #T_a4f2c_row9_col16 {\n",
       "  background-color: #6282ea;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row1_col2, #T_a4f2c_row3_col9, #T_a4f2c_row5_col9, #T_a4f2c_row7_col9, #T_a4f2c_row8_col1, #T_a4f2c_row11_col1, #T_a4f2c_row12_col1, #T_a4f2c_row14_col1, #T_a4f2c_row14_col9 {\n",
       "  background-color: #8db0fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row1_col3, #T_a4f2c_row9_col4, #T_a4f2c_row14_col0 {\n",
       "  background-color: #5d7ce6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row1_col4, #T_a4f2c_row7_col3, #T_a4f2c_row12_col3, #T_a4f2c_row14_col4, #T_a4f2c_row16_col4 {\n",
       "  background-color: #5e7de7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row1_col5, #T_a4f2c_row1_col15, #T_a4f2c_row2_col15 {\n",
       "  background-color: #4c66d6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row1_col6 {\n",
       "  background-color: #7295f4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row1_col7, #T_a4f2c_row2_col4, #T_a4f2c_row9_col12, #T_a4f2c_row12_col7 {\n",
       "  background-color: #6384eb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row1_col8, #T_a4f2c_row1_col11, #T_a4f2c_row6_col8, #T_a4f2c_row6_col11, #T_a4f2c_row7_col11, #T_a4f2c_row16_col11 {\n",
       "  background-color: #a3c2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row1_col12, #T_a4f2c_row2_col16, #T_a4f2c_row3_col12, #T_a4f2c_row5_col16, #T_a4f2c_row6_col0, #T_a4f2c_row6_col7, #T_a4f2c_row6_col12, #T_a4f2c_row10_col7, #T_a4f2c_row11_col7, #T_a4f2c_row13_col7, #T_a4f2c_row14_col12, #T_a4f2c_row14_col16, #T_a4f2c_row15_col16 {\n",
       "  background-color: #6687ed;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row1_col14, #T_a4f2c_row2_col13, #T_a4f2c_row3_col14, #T_a4f2c_row5_col14, #T_a4f2c_row6_col13, #T_a4f2c_row7_col13, #T_a4f2c_row7_col14, #T_a4f2c_row9_col14, #T_a4f2c_row13_col4 {\n",
       "  background-color: #7699f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row1_col16, #T_a4f2c_row2_col12, #T_a4f2c_row4_col12, #T_a4f2c_row10_col12, #T_a4f2c_row13_col12, #T_a4f2c_row15_col7 {\n",
       "  background-color: #6788ee;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row2_col1, #T_a4f2c_row3_col2, #T_a4f2c_row4_col1, #T_a4f2c_row5_col2, #T_a4f2c_row6_col1, #T_a4f2c_row7_col1, #T_a4f2c_row8_col9, #T_a4f2c_row11_col9, #T_a4f2c_row13_col1, #T_a4f2c_row13_col9, #T_a4f2c_row15_col9, #T_a4f2c_row15_col10 {\n",
       "  background-color: #8caffe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row2_col3, #T_a4f2c_row6_col3, #T_a4f2c_row9_col0 {\n",
       "  background-color: #5b7ae5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row2_col5, #T_a4f2c_row7_col15 {\n",
       "  background-color: #4b64d5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row2_col6 {\n",
       "  background-color: #f7b79b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row2_col8, #T_a4f2c_row2_col11, #T_a4f2c_row10_col8, #T_a4f2c_row14_col8 {\n",
       "  background-color: #a5c3fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row2_col9, #T_a4f2c_row3_col10, #T_a4f2c_row5_col10, #T_a4f2c_row6_col9, #T_a4f2c_row7_col2, #T_a4f2c_row10_col1, #T_a4f2c_row10_col9, #T_a4f2c_row11_col5, #T_a4f2c_row12_col10, #T_a4f2c_row15_col1, #T_a4f2c_row16_col10 {\n",
       "  background-color: #8fb1fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row3_col4, #T_a4f2c_row4_col3, #T_a4f2c_row7_col5, #T_a4f2c_row9_col5, #T_a4f2c_row14_col5, #T_a4f2c_row14_col6 {\n",
       "  background-color: #4f69d9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row3_col5 {\n",
       "  background-color: #ef886b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row3_col6, #T_a4f2c_row5_col6, #T_a4f2c_row11_col14 {\n",
       "  background-color: #6f92f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row3_col11 {\n",
       "  background-color: #cedaeb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row3_col15, #T_a4f2c_row9_col3, #T_a4f2c_row10_col3, #T_a4f2c_row12_col4, #T_a4f2c_row14_col3, #T_a4f2c_row16_col0, #T_a4f2c_row16_col3 {\n",
       "  background-color: #5f7fe8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row4_col2 {\n",
       "  background-color: #93b5fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row4_col5, #T_a4f2c_row6_col14, #T_a4f2c_row8_col3 {\n",
       "  background-color: #516ddb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row4_col8, #T_a4f2c_row11_col4 {\n",
       "  background-color: #82a6fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row4_col11 {\n",
       "  background-color: #bfd3f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row4_col15, #T_a4f2c_row12_col15 {\n",
       "  background-color: #dcdddd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row5_col0, #T_a4f2c_row10_col5 {\n",
       "  background-color: #4e68d8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row5_col3 {\n",
       "  background-color: #ed8366;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row5_col8, #T_a4f2c_row8_col10, #T_a4f2c_row9_col10, #T_a4f2c_row15_col2 {\n",
       "  background-color: #92b4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row5_col11 {\n",
       "  background-color: #d1dae9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row5_col12, #T_a4f2c_row6_col4 {\n",
       "  background-color: #6180e9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row5_col13 {\n",
       "  background-color: #b5cdfa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row6_col2 {\n",
       "  background-color: #f7ac8e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row6_col5, #T_a4f2c_row6_col15, #T_a4f2c_row16_col15 {\n",
       "  background-color: #4a63d3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row6_col10 {\n",
       "  background-color: #5875e1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row7_col8, #T_a4f2c_row9_col8, #T_a4f2c_row9_col11 {\n",
       "  background-color: #a2c1ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row8_col6, #T_a4f2c_row9_col13, #T_a4f2c_row11_col0, #T_a4f2c_row11_col6, #T_a4f2c_row12_col6, #T_a4f2c_row15_col3, #T_a4f2c_row15_col6, #T_a4f2c_row16_col6 {\n",
       "  background-color: #7396f5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row8_col12 {\n",
       "  background-color: #f08a6c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row8_col14, #T_a4f2c_row16_col14 {\n",
       "  background-color: #779af7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row8_col15 {\n",
       "  background-color: #bad0f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row8_col16, #T_a4f2c_row12_col16 {\n",
       "  background-color: #688aef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row9_col15 {\n",
       "  background-color: #485fd1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row10_col0, #T_a4f2c_row10_col4 {\n",
       "  background-color: #5977e3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row10_col11 {\n",
       "  background-color: #9dbdff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row10_col13, #T_a4f2c_row13_col8, #T_a4f2c_row13_col14, #T_a4f2c_row14_col13 {\n",
       "  background-color: #6e90f2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row10_col14 {\n",
       "  background-color: #f6bda2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row10_col15, #T_a4f2c_row13_col15 {\n",
       "  background-color: #465ecf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row11_col3 {\n",
       "  background-color: #9abbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row11_col10 {\n",
       "  background-color: #88abfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row11_col13 {\n",
       "  background-color: #f08b6e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row12_col5, #T_a4f2c_row14_col15 {\n",
       "  background-color: #4961d2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row12_col8 {\n",
       "  background-color: #e8765c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row12_col11 {\n",
       "  background-color: #7b9ff9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row13_col3 {\n",
       "  background-color: #a1c0ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row13_col5, #T_a4f2c_row15_col11 {\n",
       "  background-color: #96b7ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row13_col11 {\n",
       "  background-color: #ea7b60;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row14_col2 {\n",
       "  background-color: #5572df;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row14_col10 {\n",
       "  background-color: #f7b396;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row14_col11 {\n",
       "  background-color: #9ebeff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row15_col0 {\n",
       "  background-color: #bed2f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row15_col4 {\n",
       "  background-color: #e5d8d1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row15_col8 {\n",
       "  background-color: #ead5c9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row15_col12 {\n",
       "  background-color: #e8d6cc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a4f2c_row15_col13 {\n",
       "  background-color: #7093f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row16_col5 {\n",
       "  background-color: #506bda;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a4f2c_row16_col8 {\n",
       "  background-color: #a6c4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a4f2c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a4f2c_level0_col0\" class=\"col_heading level0 col0\" >E1</th>\n",
       "      <th id=\"T_a4f2c_level0_col1\" class=\"col_heading level0 col1\" >px1</th>\n",
       "      <th id=\"T_a4f2c_level0_col2\" class=\"col_heading level0 col2\" >py1</th>\n",
       "      <th id=\"T_a4f2c_level0_col3\" class=\"col_heading level0 col3\" >pz1</th>\n",
       "      <th id=\"T_a4f2c_level0_col4\" class=\"col_heading level0 col4\" >pt1</th>\n",
       "      <th id=\"T_a4f2c_level0_col5\" class=\"col_heading level0 col5\" >eta1</th>\n",
       "      <th id=\"T_a4f2c_level0_col6\" class=\"col_heading level0 col6\" >phi1</th>\n",
       "      <th id=\"T_a4f2c_level0_col7\" class=\"col_heading level0 col7\" >Q1</th>\n",
       "      <th id=\"T_a4f2c_level0_col8\" class=\"col_heading level0 col8\" >E2</th>\n",
       "      <th id=\"T_a4f2c_level0_col9\" class=\"col_heading level0 col9\" >px2</th>\n",
       "      <th id=\"T_a4f2c_level0_col10\" class=\"col_heading level0 col10\" >py2</th>\n",
       "      <th id=\"T_a4f2c_level0_col11\" class=\"col_heading level0 col11\" >pz2</th>\n",
       "      <th id=\"T_a4f2c_level0_col12\" class=\"col_heading level0 col12\" >pt2</th>\n",
       "      <th id=\"T_a4f2c_level0_col13\" class=\"col_heading level0 col13\" >eta2</th>\n",
       "      <th id=\"T_a4f2c_level0_col14\" class=\"col_heading level0 col14\" >phi2</th>\n",
       "      <th id=\"T_a4f2c_level0_col15\" class=\"col_heading level0 col15\" >M</th>\n",
       "      <th id=\"T_a4f2c_level0_col16\" class=\"col_heading level0 col16\" >Q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a4f2c_level0_row0\" class=\"row_heading level0 row0\" >E1</th>\n",
       "      <td id=\"T_a4f2c_row0_col0\" class=\"data row0 col0\" >1.000</td>\n",
       "      <td id=\"T_a4f2c_row0_col1\" class=\"data row0 col1\" >0.010</td>\n",
       "      <td id=\"T_a4f2c_row0_col2\" class=\"data row0 col2\" >0.034</td>\n",
       "      <td id=\"T_a4f2c_row0_col3\" class=\"data row0 col3\" >-0.135</td>\n",
       "      <td id=\"T_a4f2c_row0_col4\" class=\"data row0 col4\" >0.708</td>\n",
       "      <td id=\"T_a4f2c_row0_col5\" class=\"data row0 col5\" >-0.062</td>\n",
       "      <td id=\"T_a4f2c_row0_col6\" class=\"data row0 col6\" >0.022</td>\n",
       "      <td id=\"T_a4f2c_row0_col7\" class=\"data row0 col7\" >0.017</td>\n",
       "      <td id=\"T_a4f2c_row0_col8\" class=\"data row0 col8\" >-0.134</td>\n",
       "      <td id=\"T_a4f2c_row0_col9\" class=\"data row0 col9\" >-0.013</td>\n",
       "      <td id=\"T_a4f2c_row0_col10\" class=\"data row0 col10\" >-0.024</td>\n",
       "      <td id=\"T_a4f2c_row0_col11\" class=\"data row0 col11\" >0.065</td>\n",
       "      <td id=\"T_a4f2c_row0_col12\" class=\"data row0 col12\" >-0.126</td>\n",
       "      <td id=\"T_a4f2c_row0_col13\" class=\"data row0 col13\" >0.017</td>\n",
       "      <td id=\"T_a4f2c_row0_col14\" class=\"data row0 col14\" >-0.007</td>\n",
       "      <td id=\"T_a4f2c_row0_col15\" class=\"data row0 col15\" >0.308</td>\n",
       "      <td id=\"T_a4f2c_row0_col16\" class=\"data row0 col16\" >0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a4f2c_level0_row1\" class=\"row_heading level0 row1\" >px1</th>\n",
       "      <td id=\"T_a4f2c_row1_col0\" class=\"data row1 col0\" >0.010</td>\n",
       "      <td id=\"T_a4f2c_row1_col1\" class=\"data row1 col1\" >1.000</td>\n",
       "      <td id=\"T_a4f2c_row1_col2\" class=\"data row1 col2\" >-0.008</td>\n",
       "      <td id=\"T_a4f2c_row1_col3\" class=\"data row1 col3\" >-0.010</td>\n",
       "      <td id=\"T_a4f2c_row1_col4\" class=\"data row1 col4\" >-0.004</td>\n",
       "      <td id=\"T_a4f2c_row1_col5\" class=\"data row1 col5\" >-0.009</td>\n",
       "      <td id=\"T_a4f2c_row1_col6\" class=\"data row1 col6\" >-0.007</td>\n",
       "      <td id=\"T_a4f2c_row1_col7\" class=\"data row1 col7\" >-0.005</td>\n",
       "      <td id=\"T_a4f2c_row1_col8\" class=\"data row1 col8\" >-0.003</td>\n",
       "      <td id=\"T_a4f2c_row1_col9\" class=\"data row1 col9\" >-0.338</td>\n",
       "      <td id=\"T_a4f2c_row1_col10\" class=\"data row1 col10\" >0.002</td>\n",
       "      <td id=\"T_a4f2c_row1_col11\" class=\"data row1 col11\" >-0.003</td>\n",
       "      <td id=\"T_a4f2c_row1_col12\" class=\"data row1 col12\" >-0.002</td>\n",
       "      <td id=\"T_a4f2c_row1_col13\" class=\"data row1 col13\" >-0.004</td>\n",
       "      <td id=\"T_a4f2c_row1_col14\" class=\"data row1 col14\" >0.001</td>\n",
       "      <td id=\"T_a4f2c_row1_col15\" class=\"data row1 col15\" >0.006</td>\n",
       "      <td id=\"T_a4f2c_row1_col16\" class=\"data row1 col16\" >0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a4f2c_level0_row2\" class=\"row_heading level0 row2\" >py1</th>\n",
       "      <td id=\"T_a4f2c_row2_col0\" class=\"data row2 col0\" >0.034</td>\n",
       "      <td id=\"T_a4f2c_row2_col1\" class=\"data row2 col1\" >-0.008</td>\n",
       "      <td id=\"T_a4f2c_row2_col2\" class=\"data row2 col2\" >1.000</td>\n",
       "      <td id=\"T_a4f2c_row2_col3\" class=\"data row2 col3\" >-0.013</td>\n",
       "      <td id=\"T_a4f2c_row2_col4\" class=\"data row2 col4\" >0.013</td>\n",
       "      <td id=\"T_a4f2c_row2_col5\" class=\"data row2 col5\" >-0.012</td>\n",
       "      <td id=\"T_a4f2c_row2_col6\" class=\"data row2 col6\" >0.599</td>\n",
       "      <td id=\"T_a4f2c_row2_col7\" class=\"data row2 col7\" >-0.003</td>\n",
       "      <td id=\"T_a4f2c_row2_col8\" class=\"data row2 col8\" >0.002</td>\n",
       "      <td id=\"T_a4f2c_row2_col9\" class=\"data row2 col9\" >0.003</td>\n",
       "      <td id=\"T_a4f2c_row2_col10\" class=\"data row2 col10\" >-0.347</td>\n",
       "      <td id=\"T_a4f2c_row2_col11\" class=\"data row2 col11\" >0.002</td>\n",
       "      <td id=\"T_a4f2c_row2_col12\" class=\"data row2 col12\" >0.001</td>\n",
       "      <td id=\"T_a4f2c_row2_col13\" class=\"data row2 col13\" >0.002</td>\n",
       "      <td id=\"T_a4f2c_row2_col14\" class=\"data row2 col14\" >-0.227</td>\n",
       "      <td id=\"T_a4f2c_row2_col15\" class=\"data row2 col15\" >0.006</td>\n",
       "      <td id=\"T_a4f2c_row2_col16\" class=\"data row2 col16\" >0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a4f2c_level0_row3\" class=\"row_heading level0 row3\" >pz1</th>\n",
       "      <td id=\"T_a4f2c_row3_col0\" class=\"data row3 col0\" >-0.135</td>\n",
       "      <td id=\"T_a4f2c_row3_col1\" class=\"data row3 col1\" >-0.010</td>\n",
       "      <td id=\"T_a4f2c_row3_col2\" class=\"data row3 col2\" >-0.013</td>\n",
       "      <td id=\"T_a4f2c_row3_col3\" class=\"data row3 col3\" >1.000</td>\n",
       "      <td id=\"T_a4f2c_row3_col4\" class=\"data row3 col4\" >-0.057</td>\n",
       "      <td id=\"T_a4f2c_row3_col5\" class=\"data row3 col5\" >0.777</td>\n",
       "      <td id=\"T_a4f2c_row3_col6\" class=\"data row3 col6\" >-0.014</td>\n",
       "      <td id=\"T_a4f2c_row3_col7\" class=\"data row3 col7\" >-0.004</td>\n",
       "      <td id=\"T_a4f2c_row3_col8\" class=\"data row3 col8\" >-0.047</td>\n",
       "      <td id=\"T_a4f2c_row3_col9\" class=\"data row3 col9\" >0.001</td>\n",
       "      <td id=\"T_a4f2c_row3_col10\" class=\"data row3 col10\" >-0.001</td>\n",
       "      <td id=\"T_a4f2c_row3_col11\" class=\"data row3 col11\" >0.190</td>\n",
       "      <td id=\"T_a4f2c_row3_col12\" class=\"data row3 col12\" >-0.004</td>\n",
       "      <td id=\"T_a4f2c_row3_col13\" class=\"data row3 col13\" >0.212</td>\n",
       "      <td id=\"T_a4f2c_row3_col14\" class=\"data row3 col14\" >0.002</td>\n",
       "      <td id=\"T_a4f2c_row3_col15\" class=\"data row3 col15\" >0.068</td>\n",
       "      <td id=\"T_a4f2c_row3_col16\" class=\"data row3 col16\" >-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a4f2c_level0_row4\" class=\"row_heading level0 row4\" >pt1</th>\n",
       "      <td id=\"T_a4f2c_row4_col0\" class=\"data row4 col0\" >0.708</td>\n",
       "      <td id=\"T_a4f2c_row4_col1\" class=\"data row4 col1\" >-0.004</td>\n",
       "      <td id=\"T_a4f2c_row4_col2\" class=\"data row4 col2\" >0.013</td>\n",
       "      <td id=\"T_a4f2c_row4_col3\" class=\"data row4 col3\" >-0.057</td>\n",
       "      <td id=\"T_a4f2c_row4_col4\" class=\"data row4 col4\" >1.000</td>\n",
       "      <td id=\"T_a4f2c_row4_col5\" class=\"data row4 col5\" >0.007</td>\n",
       "      <td id=\"T_a4f2c_row4_col6\" class=\"data row4 col6\" >0.005</td>\n",
       "      <td id=\"T_a4f2c_row4_col7\" class=\"data row4 col7\" >0.016</td>\n",
       "      <td id=\"T_a4f2c_row4_col8\" class=\"data row4 col8\" >-0.136</td>\n",
       "      <td id=\"T_a4f2c_row4_col9\" class=\"data row4 col9\" >-0.010</td>\n",
       "      <td id=\"T_a4f2c_row4_col10\" class=\"data row4 col10\" >-0.024</td>\n",
       "      <td id=\"T_a4f2c_row4_col11\" class=\"data row4 col11\" >0.116</td>\n",
       "      <td id=\"T_a4f2c_row4_col12\" class=\"data row4 col12\" >-0.001</td>\n",
       "      <td id=\"T_a4f2c_row4_col13\" class=\"data row4 col13\" >0.073</td>\n",
       "      <td id=\"T_a4f2c_row4_col14\" class=\"data row4 col14\" >-0.006</td>\n",
       "      <td id=\"T_a4f2c_row4_col15\" class=\"data row4 col15\" >0.468</td>\n",
       "      <td id=\"T_a4f2c_row4_col16\" class=\"data row4 col16\" >-0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a4f2c_level0_row5\" class=\"row_heading level0 row5\" >eta1</th>\n",
       "      <td id=\"T_a4f2c_row5_col0\" class=\"data row5 col0\" >-0.062</td>\n",
       "      <td id=\"T_a4f2c_row5_col1\" class=\"data row5 col1\" >-0.009</td>\n",
       "      <td id=\"T_a4f2c_row5_col2\" class=\"data row5 col2\" >-0.012</td>\n",
       "      <td id=\"T_a4f2c_row5_col3\" class=\"data row5 col3\" >0.777</td>\n",
       "      <td id=\"T_a4f2c_row5_col4\" class=\"data row5 col4\" >0.007</td>\n",
       "      <td id=\"T_a4f2c_row5_col5\" class=\"data row5 col5\" >1.000</td>\n",
       "      <td id=\"T_a4f2c_row5_col6\" class=\"data row5 col6\" >-0.016</td>\n",
       "      <td id=\"T_a4f2c_row5_col7\" class=\"data row5 col7\" >-0.002</td>\n",
       "      <td id=\"T_a4f2c_row5_col8\" class=\"data row5 col8\" >-0.074</td>\n",
       "      <td id=\"T_a4f2c_row5_col9\" class=\"data row5 col9\" >-0.001</td>\n",
       "      <td id=\"T_a4f2c_row5_col10\" class=\"data row5 col10\" >-0.005</td>\n",
       "      <td id=\"T_a4f2c_row5_col11\" class=\"data row5 col11\" >0.202</td>\n",
       "      <td id=\"T_a4f2c_row5_col12\" class=\"data row5 col12\" >-0.020</td>\n",
       "      <td id=\"T_a4f2c_row5_col13\" class=\"data row5 col13\" >0.224</td>\n",
       "      <td id=\"T_a4f2c_row5_col14\" class=\"data row5 col14\" >-0.000</td>\n",
       "      <td id=\"T_a4f2c_row5_col15\" class=\"data row5 col15\" >0.136</td>\n",
       "      <td id=\"T_a4f2c_row5_col16\" class=\"data row5 col16\" >0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a4f2c_level0_row6\" class=\"row_heading level0 row6\" >phi1</th>\n",
       "      <td id=\"T_a4f2c_row6_col0\" class=\"data row6 col0\" >0.022</td>\n",
       "      <td id=\"T_a4f2c_row6_col1\" class=\"data row6 col1\" >-0.007</td>\n",
       "      <td id=\"T_a4f2c_row6_col2\" class=\"data row6 col2\" >0.599</td>\n",
       "      <td id=\"T_a4f2c_row6_col3\" class=\"data row6 col3\" >-0.014</td>\n",
       "      <td id=\"T_a4f2c_row6_col4\" class=\"data row6 col4\" >0.005</td>\n",
       "      <td id=\"T_a4f2c_row6_col5\" class=\"data row6 col5\" >-0.016</td>\n",
       "      <td id=\"T_a4f2c_row6_col6\" class=\"data row6 col6\" >1.000</td>\n",
       "      <td id=\"T_a4f2c_row6_col7\" class=\"data row6 col7\" >0.002</td>\n",
       "      <td id=\"T_a4f2c_row6_col8\" class=\"data row6 col8\" >0.001</td>\n",
       "      <td id=\"T_a4f2c_row6_col9\" class=\"data row6 col9\" >0.004</td>\n",
       "      <td id=\"T_a4f2c_row6_col10\" class=\"data row6 col10\" >-0.217</td>\n",
       "      <td id=\"T_a4f2c_row6_col11\" class=\"data row6 col11\" >-0.000</td>\n",
       "      <td id=\"T_a4f2c_row6_col12\" class=\"data row6 col12\" >-0.002</td>\n",
       "      <td id=\"T_a4f2c_row6_col13\" class=\"data row6 col13\" >0.002</td>\n",
       "      <td id=\"T_a4f2c_row6_col14\" class=\"data row6 col14\" >-0.132</td>\n",
       "      <td id=\"T_a4f2c_row6_col15\" class=\"data row6 col15\" >0.002</td>\n",
       "      <td id=\"T_a4f2c_row6_col16\" class=\"data row6 col16\" >-0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a4f2c_level0_row7\" class=\"row_heading level0 row7\" >Q1</th>\n",
       "      <td id=\"T_a4f2c_row7_col0\" class=\"data row7 col0\" >0.017</td>\n",
       "      <td id=\"T_a4f2c_row7_col1\" class=\"data row7 col1\" >-0.005</td>\n",
       "      <td id=\"T_a4f2c_row7_col2\" class=\"data row7 col2\" >-0.003</td>\n",
       "      <td id=\"T_a4f2c_row7_col3\" class=\"data row7 col3\" >-0.004</td>\n",
       "      <td id=\"T_a4f2c_row7_col4\" class=\"data row7 col4\" >0.016</td>\n",
       "      <td id=\"T_a4f2c_row7_col5\" class=\"data row7 col5\" >-0.002</td>\n",
       "      <td id=\"T_a4f2c_row7_col6\" class=\"data row7 col6\" >0.002</td>\n",
       "      <td id=\"T_a4f2c_row7_col7\" class=\"data row7 col7\" >1.000</td>\n",
       "      <td id=\"T_a4f2c_row7_col8\" class=\"data row7 col8\" >-0.009</td>\n",
       "      <td id=\"T_a4f2c_row7_col9\" class=\"data row7 col9\" >-0.003</td>\n",
       "      <td id=\"T_a4f2c_row7_col10\" class=\"data row7 col10\" >0.005</td>\n",
       "      <td id=\"T_a4f2c_row7_col11\" class=\"data row7 col11\" >0.001</td>\n",
       "      <td id=\"T_a4f2c_row7_col12\" class=\"data row7 col12\" >-0.008</td>\n",
       "      <td id=\"T_a4f2c_row7_col13\" class=\"data row7 col13\" >0.001</td>\n",
       "      <td id=\"T_a4f2c_row7_col14\" class=\"data row7 col14\" >-0.001</td>\n",
       "      <td id=\"T_a4f2c_row7_col15\" class=\"data row7 col15\" >0.006</td>\n",
       "      <td id=\"T_a4f2c_row7_col16\" class=\"data row7 col16\" >-0.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a4f2c_level0_row8\" class=\"row_heading level0 row8\" >E2</th>\n",
       "      <td id=\"T_a4f2c_row8_col0\" class=\"data row8 col0\" >-0.134</td>\n",
       "      <td id=\"T_a4f2c_row8_col1\" class=\"data row8 col1\" >-0.003</td>\n",
       "      <td id=\"T_a4f2c_row8_col2\" class=\"data row8 col2\" >0.002</td>\n",
       "      <td id=\"T_a4f2c_row8_col3\" class=\"data row8 col3\" >-0.047</td>\n",
       "      <td id=\"T_a4f2c_row8_col4\" class=\"data row8 col4\" >-0.136</td>\n",
       "      <td id=\"T_a4f2c_row8_col5\" class=\"data row8 col5\" >-0.074</td>\n",
       "      <td id=\"T_a4f2c_row8_col6\" class=\"data row8 col6\" >0.001</td>\n",
       "      <td id=\"T_a4f2c_row8_col7\" class=\"data row8 col7\" >-0.009</td>\n",
       "      <td id=\"T_a4f2c_row8_col8\" class=\"data row8 col8\" >1.000</td>\n",
       "      <td id=\"T_a4f2c_row8_col9\" class=\"data row8 col9\" >-0.004</td>\n",
       "      <td id=\"T_a4f2c_row8_col10\" class=\"data row8 col10\" >0.007</td>\n",
       "      <td id=\"T_a4f2c_row8_col11\" class=\"data row8 col11\" >-0.460</td>\n",
       "      <td id=\"T_a4f2c_row8_col12\" class=\"data row8 col12\" >0.753</td>\n",
       "      <td id=\"T_a4f2c_row8_col13\" class=\"data row8 col13\" >-0.224</td>\n",
       "      <td id=\"T_a4f2c_row8_col14\" class=\"data row8 col14\" >0.007</td>\n",
       "      <td id=\"T_a4f2c_row8_col15\" class=\"data row8 col15\" >0.347</td>\n",
       "      <td id=\"T_a4f2c_row8_col16\" class=\"data row8 col16\" >0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a4f2c_level0_row9\" class=\"row_heading level0 row9\" >px2</th>\n",
       "      <td id=\"T_a4f2c_row9_col0\" class=\"data row9 col0\" >-0.013</td>\n",
       "      <td id=\"T_a4f2c_row9_col1\" class=\"data row9 col1\" >-0.338</td>\n",
       "      <td id=\"T_a4f2c_row9_col2\" class=\"data row9 col2\" >0.003</td>\n",
       "      <td id=\"T_a4f2c_row9_col3\" class=\"data row9 col3\" >0.001</td>\n",
       "      <td id=\"T_a4f2c_row9_col4\" class=\"data row9 col4\" >-0.010</td>\n",
       "      <td id=\"T_a4f2c_row9_col5\" class=\"data row9 col5\" >-0.001</td>\n",
       "      <td id=\"T_a4f2c_row9_col6\" class=\"data row9 col6\" >0.004</td>\n",
       "      <td id=\"T_a4f2c_row9_col7\" class=\"data row9 col7\" >-0.003</td>\n",
       "      <td id=\"T_a4f2c_row9_col8\" class=\"data row9 col8\" >-0.004</td>\n",
       "      <td id=\"T_a4f2c_row9_col9\" class=\"data row9 col9\" >1.000</td>\n",
       "      <td id=\"T_a4f2c_row9_col10\" class=\"data row9 col10\" >0.006</td>\n",
       "      <td id=\"T_a4f2c_row9_col11\" class=\"data row9 col11\" >-0.006</td>\n",
       "      <td id=\"T_a4f2c_row9_col12\" class=\"data row9 col12\" >-0.011</td>\n",
       "      <td id=\"T_a4f2c_row9_col13\" class=\"data row9 col13\" >-0.006</td>\n",
       "      <td id=\"T_a4f2c_row9_col14\" class=\"data row9 col14\" >-0.002</td>\n",
       "      <td id=\"T_a4f2c_row9_col15\" class=\"data row9 col15\" >-0.008</td>\n",
       "      <td id=\"T_a4f2c_row9_col16\" class=\"data row9 col16\" >-0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a4f2c_level0_row10\" class=\"row_heading level0 row10\" >py2</th>\n",
       "      <td id=\"T_a4f2c_row10_col0\" class=\"data row10 col0\" >-0.024</td>\n",
       "      <td id=\"T_a4f2c_row10_col1\" class=\"data row10 col1\" >0.002</td>\n",
       "      <td id=\"T_a4f2c_row10_col2\" class=\"data row10 col2\" >-0.347</td>\n",
       "      <td id=\"T_a4f2c_row10_col3\" class=\"data row10 col3\" >-0.001</td>\n",
       "      <td id=\"T_a4f2c_row10_col4\" class=\"data row10 col4\" >-0.024</td>\n",
       "      <td id=\"T_a4f2c_row10_col5\" class=\"data row10 col5\" >-0.005</td>\n",
       "      <td id=\"T_a4f2c_row10_col6\" class=\"data row10 col6\" >-0.217</td>\n",
       "      <td id=\"T_a4f2c_row10_col7\" class=\"data row10 col7\" >0.005</td>\n",
       "      <td id=\"T_a4f2c_row10_col8\" class=\"data row10 col8\" >0.007</td>\n",
       "      <td id=\"T_a4f2c_row10_col9\" class=\"data row10 col9\" >0.006</td>\n",
       "      <td id=\"T_a4f2c_row10_col10\" class=\"data row10 col10\" >1.000</td>\n",
       "      <td id=\"T_a4f2c_row10_col11\" class=\"data row10 col11\" >-0.027</td>\n",
       "      <td id=\"T_a4f2c_row10_col12\" class=\"data row10 col12\" >-0.001</td>\n",
       "      <td id=\"T_a4f2c_row10_col13\" class=\"data row10 col13\" >-0.024</td>\n",
       "      <td id=\"T_a4f2c_row10_col14\" class=\"data row10 col14\" >0.572</td>\n",
       "      <td id=\"T_a4f2c_row10_col15\" class=\"data row10 col15\" >-0.012</td>\n",
       "      <td id=\"T_a4f2c_row10_col16\" class=\"data row10 col16\" >-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a4f2c_level0_row11\" class=\"row_heading level0 row11\" >pz2</th>\n",
       "      <td id=\"T_a4f2c_row11_col0\" class=\"data row11 col0\" >0.065</td>\n",
       "      <td id=\"T_a4f2c_row11_col1\" class=\"data row11 col1\" >-0.003</td>\n",
       "      <td id=\"T_a4f2c_row11_col2\" class=\"data row11 col2\" >0.002</td>\n",
       "      <td id=\"T_a4f2c_row11_col3\" class=\"data row11 col3\" >0.190</td>\n",
       "      <td id=\"T_a4f2c_row11_col4\" class=\"data row11 col4\" >0.116</td>\n",
       "      <td id=\"T_a4f2c_row11_col5\" class=\"data row11 col5\" >0.202</td>\n",
       "      <td id=\"T_a4f2c_row11_col6\" class=\"data row11 col6\" >-0.000</td>\n",
       "      <td id=\"T_a4f2c_row11_col7\" class=\"data row11 col7\" >0.001</td>\n",
       "      <td id=\"T_a4f2c_row11_col8\" class=\"data row11 col8\" >-0.460</td>\n",
       "      <td id=\"T_a4f2c_row11_col9\" class=\"data row11 col9\" >-0.006</td>\n",
       "      <td id=\"T_a4f2c_row11_col10\" class=\"data row11 col10\" >-0.027</td>\n",
       "      <td id=\"T_a4f2c_row11_col11\" class=\"data row11 col11\" >1.000</td>\n",
       "      <td id=\"T_a4f2c_row11_col12\" class=\"data row11 col12\" >-0.165</td>\n",
       "      <td id=\"T_a4f2c_row11_col13\" class=\"data row11 col13\" >0.734</td>\n",
       "      <td id=\"T_a4f2c_row11_col14\" class=\"data row11 col14\" >-0.024</td>\n",
       "      <td id=\"T_a4f2c_row11_col15\" class=\"data row11 col15\" >-0.056</td>\n",
       "      <td id=\"T_a4f2c_row11_col16\" class=\"data row11 col16\" >-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a4f2c_level0_row12\" class=\"row_heading level0 row12\" >pt2</th>\n",
       "      <td id=\"T_a4f2c_row12_col0\" class=\"data row12 col0\" >-0.126</td>\n",
       "      <td id=\"T_a4f2c_row12_col1\" class=\"data row12 col1\" >-0.002</td>\n",
       "      <td id=\"T_a4f2c_row12_col2\" class=\"data row12 col2\" >0.001</td>\n",
       "      <td id=\"T_a4f2c_row12_col3\" class=\"data row12 col3\" >-0.004</td>\n",
       "      <td id=\"T_a4f2c_row12_col4\" class=\"data row12 col4\" >-0.001</td>\n",
       "      <td id=\"T_a4f2c_row12_col5\" class=\"data row12 col5\" >-0.020</td>\n",
       "      <td id=\"T_a4f2c_row12_col6\" class=\"data row12 col6\" >-0.002</td>\n",
       "      <td id=\"T_a4f2c_row12_col7\" class=\"data row12 col7\" >-0.008</td>\n",
       "      <td id=\"T_a4f2c_row12_col8\" class=\"data row12 col8\" >0.753</td>\n",
       "      <td id=\"T_a4f2c_row12_col9\" class=\"data row12 col9\" >-0.011</td>\n",
       "      <td id=\"T_a4f2c_row12_col10\" class=\"data row12 col10\" >-0.001</td>\n",
       "      <td id=\"T_a4f2c_row12_col11\" class=\"data row12 col11\" >-0.165</td>\n",
       "      <td id=\"T_a4f2c_row12_col12\" class=\"data row12 col12\" >1.000</td>\n",
       "      <td id=\"T_a4f2c_row12_col13\" class=\"data row12 col13\" >-0.001</td>\n",
       "      <td id=\"T_a4f2c_row12_col14\" class=\"data row12 col14\" >-0.002</td>\n",
       "      <td id=\"T_a4f2c_row12_col15\" class=\"data row12 col15\" >0.468</td>\n",
       "      <td id=\"T_a4f2c_row12_col16\" class=\"data row12 col16\" >0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a4f2c_level0_row13\" class=\"row_heading level0 row13\" >eta2</th>\n",
       "      <td id=\"T_a4f2c_row13_col0\" class=\"data row13 col0\" >0.017</td>\n",
       "      <td id=\"T_a4f2c_row13_col1\" class=\"data row13 col1\" >-0.004</td>\n",
       "      <td id=\"T_a4f2c_row13_col2\" class=\"data row13 col2\" >0.002</td>\n",
       "      <td id=\"T_a4f2c_row13_col3\" class=\"data row13 col3\" >0.212</td>\n",
       "      <td id=\"T_a4f2c_row13_col4\" class=\"data row13 col4\" >0.073</td>\n",
       "      <td id=\"T_a4f2c_row13_col5\" class=\"data row13 col5\" >0.224</td>\n",
       "      <td id=\"T_a4f2c_row13_col6\" class=\"data row13 col6\" >0.002</td>\n",
       "      <td id=\"T_a4f2c_row13_col7\" class=\"data row13 col7\" >0.001</td>\n",
       "      <td id=\"T_a4f2c_row13_col8\" class=\"data row13 col8\" >-0.224</td>\n",
       "      <td id=\"T_a4f2c_row13_col9\" class=\"data row13 col9\" >-0.006</td>\n",
       "      <td id=\"T_a4f2c_row13_col10\" class=\"data row13 col10\" >-0.024</td>\n",
       "      <td id=\"T_a4f2c_row13_col11\" class=\"data row13 col11\" >0.734</td>\n",
       "      <td id=\"T_a4f2c_row13_col12\" class=\"data row13 col12\" >-0.001</td>\n",
       "      <td id=\"T_a4f2c_row13_col13\" class=\"data row13 col13\" >1.000</td>\n",
       "      <td id=\"T_a4f2c_row13_col14\" class=\"data row13 col14\" >-0.027</td>\n",
       "      <td id=\"T_a4f2c_row13_col15\" class=\"data row13 col15\" >-0.015</td>\n",
       "      <td id=\"T_a4f2c_row13_col16\" class=\"data row13 col16\" >-0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a4f2c_level0_row14\" class=\"row_heading level0 row14\" >phi2</th>\n",
       "      <td id=\"T_a4f2c_row14_col0\" class=\"data row14 col0\" >-0.007</td>\n",
       "      <td id=\"T_a4f2c_row14_col1\" class=\"data row14 col1\" >0.001</td>\n",
       "      <td id=\"T_a4f2c_row14_col2\" class=\"data row14 col2\" >-0.227</td>\n",
       "      <td id=\"T_a4f2c_row14_col3\" class=\"data row14 col3\" >0.002</td>\n",
       "      <td id=\"T_a4f2c_row14_col4\" class=\"data row14 col4\" >-0.006</td>\n",
       "      <td id=\"T_a4f2c_row14_col5\" class=\"data row14 col5\" >-0.000</td>\n",
       "      <td id=\"T_a4f2c_row14_col6\" class=\"data row14 col6\" >-0.132</td>\n",
       "      <td id=\"T_a4f2c_row14_col7\" class=\"data row14 col7\" >-0.001</td>\n",
       "      <td id=\"T_a4f2c_row14_col8\" class=\"data row14 col8\" >0.007</td>\n",
       "      <td id=\"T_a4f2c_row14_col9\" class=\"data row14 col9\" >-0.002</td>\n",
       "      <td id=\"T_a4f2c_row14_col10\" class=\"data row14 col10\" >0.572</td>\n",
       "      <td id=\"T_a4f2c_row14_col11\" class=\"data row14 col11\" >-0.024</td>\n",
       "      <td id=\"T_a4f2c_row14_col12\" class=\"data row14 col12\" >-0.002</td>\n",
       "      <td id=\"T_a4f2c_row14_col13\" class=\"data row14 col13\" >-0.027</td>\n",
       "      <td id=\"T_a4f2c_row14_col14\" class=\"data row14 col14\" >1.000</td>\n",
       "      <td id=\"T_a4f2c_row14_col15\" class=\"data row14 col15\" >-0.003</td>\n",
       "      <td id=\"T_a4f2c_row14_col16\" class=\"data row14 col16\" >0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a4f2c_level0_row15\" class=\"row_heading level0 row15\" >M</th>\n",
       "      <td id=\"T_a4f2c_row15_col0\" class=\"data row15 col0\" >0.308</td>\n",
       "      <td id=\"T_a4f2c_row15_col1\" class=\"data row15 col1\" >0.006</td>\n",
       "      <td id=\"T_a4f2c_row15_col2\" class=\"data row15 col2\" >0.006</td>\n",
       "      <td id=\"T_a4f2c_row15_col3\" class=\"data row15 col3\" >0.068</td>\n",
       "      <td id=\"T_a4f2c_row15_col4\" class=\"data row15 col4\" >0.468</td>\n",
       "      <td id=\"T_a4f2c_row15_col5\" class=\"data row15 col5\" >0.136</td>\n",
       "      <td id=\"T_a4f2c_row15_col6\" class=\"data row15 col6\" >0.002</td>\n",
       "      <td id=\"T_a4f2c_row15_col7\" class=\"data row15 col7\" >0.006</td>\n",
       "      <td id=\"T_a4f2c_row15_col8\" class=\"data row15 col8\" >0.347</td>\n",
       "      <td id=\"T_a4f2c_row15_col9\" class=\"data row15 col9\" >-0.008</td>\n",
       "      <td id=\"T_a4f2c_row15_col10\" class=\"data row15 col10\" >-0.012</td>\n",
       "      <td id=\"T_a4f2c_row15_col11\" class=\"data row15 col11\" >-0.056</td>\n",
       "      <td id=\"T_a4f2c_row15_col12\" class=\"data row15 col12\" >0.468</td>\n",
       "      <td id=\"T_a4f2c_row15_col13\" class=\"data row15 col13\" >-0.015</td>\n",
       "      <td id=\"T_a4f2c_row15_col14\" class=\"data row15 col14\" >-0.003</td>\n",
       "      <td id=\"T_a4f2c_row15_col15\" class=\"data row15 col15\" >1.000</td>\n",
       "      <td id=\"T_a4f2c_row15_col16\" class=\"data row15 col16\" >0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a4f2c_level0_row16\" class=\"row_heading level0 row16\" >Q2</th>\n",
       "      <td id=\"T_a4f2c_row16_col0\" class=\"data row16 col0\" >0.000</td>\n",
       "      <td id=\"T_a4f2c_row16_col1\" class=\"data row16 col1\" >0.007</td>\n",
       "      <td id=\"T_a4f2c_row16_col2\" class=\"data row16 col2\" >0.005</td>\n",
       "      <td id=\"T_a4f2c_row16_col3\" class=\"data row16 col3\" >-0.001</td>\n",
       "      <td id=\"T_a4f2c_row16_col4\" class=\"data row16 col4\" >-0.004</td>\n",
       "      <td id=\"T_a4f2c_row16_col5\" class=\"data row16 col5\" >0.002</td>\n",
       "      <td id=\"T_a4f2c_row16_col6\" class=\"data row16 col6\" >-0.003</td>\n",
       "      <td id=\"T_a4f2c_row16_col7\" class=\"data row16 col7\" >-0.158</td>\n",
       "      <td id=\"T_a4f2c_row16_col8\" class=\"data row16 col8\" >0.011</td>\n",
       "      <td id=\"T_a4f2c_row16_col9\" class=\"data row16 col9\" >-0.009</td>\n",
       "      <td id=\"T_a4f2c_row16_col10\" class=\"data row16 col10\" >-0.001</td>\n",
       "      <td id=\"T_a4f2c_row16_col11\" class=\"data row16 col11\" >-0.001</td>\n",
       "      <td id=\"T_a4f2c_row16_col12\" class=\"data row16 col12\" >0.011</td>\n",
       "      <td id=\"T_a4f2c_row16_col13\" class=\"data row16 col13\" >-0.004</td>\n",
       "      <td id=\"T_a4f2c_row16_col14\" class=\"data row16 col14\" >0.003</td>\n",
       "      <td id=\"T_a4f2c_row16_col15\" class=\"data row16 col15\" >0.001</td>\n",
       "      <td id=\"T_a4f2c_row16_col16\" class=\"data row16 col16\" >1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1882bd2ff10>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr().style.background_gradient(cmap='coolwarm').set_precision(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6735298",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44202022",
   "metadata": {},
   "source": [
    "#### Выбросы в данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2305b",
   "metadata": {},
   "source": [
    "Изучим данные на предмет наличия выбросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb4c1469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>px1</th>\n",
       "      <th>py1</th>\n",
       "      <th>pz1</th>\n",
       "      <th>pt1</th>\n",
       "      <th>eta1</th>\n",
       "      <th>phi1</th>\n",
       "      <th>Q1</th>\n",
       "      <th>E2</th>\n",
       "      <th>px2</th>\n",
       "      <th>py2</th>\n",
       "      <th>pz2</th>\n",
       "      <th>pt2</th>\n",
       "      <th>eta2</th>\n",
       "      <th>phi2</th>\n",
       "      <th>M</th>\n",
       "      <th>Q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84915.000000</td>\n",
       "      <td>84915.000000</td>\n",
       "      <td>84915.000000</td>\n",
       "      <td>84915.000000</td>\n",
       "      <td>84915.000000</td>\n",
       "      <td>84915.000000</td>\n",
       "      <td>84915.000000</td>\n",
       "      <td>84915.000000</td>\n",
       "      <td>84915.000000</td>\n",
       "      <td>84915.000000</td>\n",
       "      <td>84915.000000</td>\n",
       "      <td>84915.000000</td>\n",
       "      <td>84915.000000</td>\n",
       "      <td>84915.000000</td>\n",
       "      <td>84915.000000</td>\n",
       "      <td>84915.000000</td>\n",
       "      <td>84915.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>37.073493</td>\n",
       "      <td>0.142215</td>\n",
       "      <td>0.215104</td>\n",
       "      <td>-4.115892</td>\n",
       "      <td>14.881585</td>\n",
       "      <td>-0.148518</td>\n",
       "      <td>0.023190</td>\n",
       "      <td>-0.001755</td>\n",
       "      <td>39.008094</td>\n",
       "      <td>-0.024558</td>\n",
       "      <td>0.163524</td>\n",
       "      <td>-14.408340</td>\n",
       "      <td>13.684794</td>\n",
       "      <td>-0.487033</td>\n",
       "      <td>0.025111</td>\n",
       "      <td>29.468604</td>\n",
       "      <td>-0.004298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>41.246495</td>\n",
       "      <td>13.744603</td>\n",
       "      <td>13.781461</td>\n",
       "      <td>51.767629</td>\n",
       "      <td>12.547636</td>\n",
       "      <td>1.442745</td>\n",
       "      <td>1.799231</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>42.945899</td>\n",
       "      <td>13.223100</td>\n",
       "      <td>13.193532</td>\n",
       "      <td>53.004257</td>\n",
       "      <td>12.714954</td>\n",
       "      <td>1.552891</td>\n",
       "      <td>1.818815</td>\n",
       "      <td>25.193575</td>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.518048</td>\n",
       "      <td>-250.587000</td>\n",
       "      <td>-126.079000</td>\n",
       "      <td>-840.987000</td>\n",
       "      <td>0.250803</td>\n",
       "      <td>-4.165380</td>\n",
       "      <td>-3.141580</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.472500</td>\n",
       "      <td>-233.730000</td>\n",
       "      <td>-134.753000</td>\n",
       "      <td>-655.396000</td>\n",
       "      <td>0.026651</td>\n",
       "      <td>-7.064790</td>\n",
       "      <td>-3.141580</td>\n",
       "      <td>2.000080</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.959955</td>\n",
       "      <td>-5.647490</td>\n",
       "      <td>-5.637360</td>\n",
       "      <td>-18.399750</td>\n",
       "      <td>3.990330</td>\n",
       "      <td>-1.351155</td>\n",
       "      <td>-1.525245</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>9.667075</td>\n",
       "      <td>-4.590990</td>\n",
       "      <td>-4.353425</td>\n",
       "      <td>-29.660000</td>\n",
       "      <td>3.587415</td>\n",
       "      <td>-2.004690</td>\n",
       "      <td>-1.555150</td>\n",
       "      <td>12.090150</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>22.466400</td>\n",
       "      <td>0.151828</td>\n",
       "      <td>0.112649</td>\n",
       "      <td>-0.970995</td>\n",
       "      <td>13.341300</td>\n",
       "      <td>-0.169535</td>\n",
       "      <td>0.037524</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>22.243600</td>\n",
       "      <td>-0.056887</td>\n",
       "      <td>0.112152</td>\n",
       "      <td>-5.164620</td>\n",
       "      <td>10.786600</td>\n",
       "      <td>-0.737446</td>\n",
       "      <td>0.040129</td>\n",
       "      <td>20.725100</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>51.028850</td>\n",
       "      <td>6.167240</td>\n",
       "      <td>6.129330</td>\n",
       "      <td>11.583950</td>\n",
       "      <td>20.435550</td>\n",
       "      <td>1.039235</td>\n",
       "      <td>1.562695</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>57.029750</td>\n",
       "      <td>4.615085</td>\n",
       "      <td>4.889965</td>\n",
       "      <td>8.080910</td>\n",
       "      <td>19.515300</td>\n",
       "      <td>0.965475</td>\n",
       "      <td>1.607640</td>\n",
       "      <td>37.746100</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>850.602000</td>\n",
       "      <td>134.539000</td>\n",
       "      <td>147.467000</td>\n",
       "      <td>513.709000</td>\n",
       "      <td>265.578000</td>\n",
       "      <td>2.622970</td>\n",
       "      <td>3.202600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>667.450000</td>\n",
       "      <td>227.330000</td>\n",
       "      <td>166.283000</td>\n",
       "      <td>623.049000</td>\n",
       "      <td>281.654000</td>\n",
       "      <td>2.609400</td>\n",
       "      <td>3.202700</td>\n",
       "      <td>109.999000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 E1           px1           py1           pz1           pt1  \\\n",
       "count  84915.000000  84915.000000  84915.000000  84915.000000  84915.000000   \n",
       "mean      37.073493      0.142215      0.215104     -4.115892     14.881585   \n",
       "std       41.246495     13.744603     13.781461     51.767629     12.547636   \n",
       "min        0.518048   -250.587000   -126.079000   -840.987000      0.250803   \n",
       "25%        8.959955     -5.647490     -5.637360    -18.399750      3.990330   \n",
       "50%       22.466400      0.151828      0.112649     -0.970995     13.341300   \n",
       "75%       51.028850      6.167240      6.129330     11.583950     20.435550   \n",
       "max      850.602000    134.539000    147.467000    513.709000    265.578000   \n",
       "\n",
       "               eta1          phi1            Q1            E2           px2  \\\n",
       "count  84915.000000  84915.000000  84915.000000  84915.000000  84915.000000   \n",
       "mean      -0.148518      0.023190     -0.001755     39.008094     -0.024558   \n",
       "std        1.442745      1.799231      1.000004     42.945899     13.223100   \n",
       "min       -4.165380     -3.141580     -1.000000      0.472500   -233.730000   \n",
       "25%       -1.351155     -1.525245     -1.000000      9.667075     -4.590990   \n",
       "50%       -0.169535      0.037524     -1.000000     22.243600     -0.056887   \n",
       "75%        1.039235      1.562695      1.000000     57.029750      4.615085   \n",
       "max        2.622970      3.202600      1.000000    667.450000    227.330000   \n",
       "\n",
       "                py2           pz2           pt2          eta2          phi2  \\\n",
       "count  84915.000000  84915.000000  84915.000000  84915.000000  84915.000000   \n",
       "mean       0.163524    -14.408340     13.684794     -0.487033      0.025111   \n",
       "std       13.193532     53.004257     12.714954      1.552891      1.818815   \n",
       "min     -134.753000   -655.396000      0.026651     -7.064790     -3.141580   \n",
       "25%       -4.353425    -29.660000      3.587415     -2.004690     -1.555150   \n",
       "50%        0.112152     -5.164620     10.786600     -0.737446      0.040129   \n",
       "75%        4.889965      8.080910     19.515300      0.965475      1.607640   \n",
       "max      166.283000    623.049000    281.654000      2.609400      3.202700   \n",
       "\n",
       "                  M            Q2  \n",
       "count  84915.000000  84915.000000  \n",
       "mean      29.468604     -0.004298  \n",
       "std       25.193575      0.999997  \n",
       "min        2.000080     -1.000000  \n",
       "25%       12.090150     -1.000000  \n",
       "50%       20.725100     -1.000000  \n",
       "75%       37.746100      1.000000  \n",
       "max      109.999000      1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6317efcc",
   "metadata": {},
   "source": [
    "Можно заметить, что у всех признаков минимальные и максимальные значения сильно отличаются от среднего значения. Чтобы узнать, являются ли подобные данные выбросом, построим гистограмму распределения признаков на примере инвариантной массы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb9a26c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='M', ylabel='Count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(df[\"M\"], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7df3443",
   "metadata": {},
   "source": [
    "На гистограмме распределения можно наглядно заметить хвост тяжёлый хвост справа. Но при этом подобный тяжёлый хвост образуют довольно большое количество элементов и, к тому же, они образуют что-то похожее на нормальное распределение. Из этого можно сделать вывод, что подобные большие значения являются не аномалией, а скорее просто особенностью в данных, которая может даже стать полезным признаком при решении нашей задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41d55d1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='px1', ylabel='Count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(df[\"px1\"], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b76cfac",
   "metadata": {},
   "source": [
    "Вообще можно заметить, что распределения большинства признаков похоже на нормальное. Это вполне логично, потому что многие физические явления можно описать нормальным распределением."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f3dbbf",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fe2950",
   "metadata": {},
   "source": [
    "При подобных распределениях данных, где значения величин признаков могут сильно отличаться друг от друга, но при этом не являются выбросами, больше подходит не удаление признаков, а нормализация данных, потому что она помогает сохранить информацию о том, что одна величина сильно больше другой, но при этом не мешает обучению моделей, например, основанных на градиентном спуске.  \n",
    "Тем не менее, были испробованы оба метода и в промежуточных решениях метод с удалением \"аномалий\" помог повысить точность. Но в финальном решении была применена нормализация (и то не для всех моделей, некоторые принимали нормализированные данные, некоторые - нет, т.к. это давало большую точность)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5942352a",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9091ff9",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be52689",
   "metadata": {},
   "source": [
    "В первую очередь, стоит заметить, что половина признаков в датасете описывают первую элементарную частицу в эксперименте, когда нам нужно предсказывать заряд второй. И единственная фича, связывающая первую и вторую частицу - это их инвариантная масса. Кстати, она довольно сильно коррелирует с другими величинами, и её можно хорошо предсказывать с помощью регрессионных МЛ-моделей.\n",
    "Поэтому я решил, что нам было бы неплохо попытаться найти отдельно массу первой и второй частицы.  \n",
    "Я не нашёл в интернете чёткой математической формулы, позволяющей из имеющихся данных точно высчитать массу первой и второй частицы. Но, по-сути, нам не так важна точность, если мы хотим использовать её для прогнозирующих моделей. Нам, скорее, важно, чтобы она отражала некую зависимость и общую взаимную корреляцию с другими признаками.  \n",
    "Поэтому я придумал следующий метод:  \n",
    "\n",
    "Во-первых, мы знаем, что саму инвариантную массу можно предсказать с довольно высокой точностью, используя имеющиеся данные. С задачей предсказания хорошо справляются как модели классического МЛя, такие как регрессионные деревья решений, К-ближайших соседей или Градиентный бустинг, так и полносвязные нейросети (в точности они не уступают, и даже превосходят некоторые другие алгоритмы). Но для моего метода я буду использовать нейросети, т.к. там проще реализовать описанный ниже алгоритм:\n",
    "\n",
    "1. Предположим, что первоначальная масса каждой частицы не зависит от массы частицы, с которой она столкнётся в ходе эксперимента (это довольно логично);   \n",
    "\n",
    "2. Построим полносвязную нейронную сеть, которая на вход будет принимать {E, p(x, y, z), eta, phi, q} частицы, а на выходе предсказывать её массу;  \n",
    "\n",
    "3. В ходе обучения нейросети, будем подавать ей на вход сперва данные для первой частицы, участвующей в эксперементе, а потом второй (главное, чтобы они были из одного эксперимента!);  \n",
    "\n",
    "4. В качестве функции ошибки для нейросети будет использоваться (M - (m1 + m2)), где m1 - предсказания модели для первой частицы, m2 - для второй, а M - инвариантная масса обеих частиц;\n",
    "\n",
    "5. После обучения модели будем считать массу для первой частицы, используя модель, а массу для второй как M - m1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03315ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.FloatTensor(df[\"M\"].values)\n",
    "X1 = torch.FloatTensor(df[['E1', 'px1', 'py1', 'pz1', 'pt1', 'eta1', 'phi1', 'Q1']].values)\n",
    "X2 = torch.FloatTensor(df[['E2', 'px2', 'py2', 'pz2', 'pt2', 'eta2', 'phi2', 'Q2']].values)\n",
    "\n",
    "def shuffle(x1, x2, y):\n",
    "    for _ in range(len(y)*3):\n",
    "        i, j = random.randint(0, len(y)-1), random.randint(0, len(y)-1)\n",
    "        x1[i], x1[j] = x1[j], x1[i]\n",
    "        x2[i], x2[j] = x2[j], x2[i]\n",
    "        y[i], y[j] = y[j], y[i]\n",
    "    return x1, x2, y\n",
    "\n",
    "\n",
    "X1, X2, y = shuffle(X1, X2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c4c4b",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "ca1e902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = int(len(y)*0.8)\n",
    "\n",
    "train_dataset, valid_dataset = (X1[:p], X2[:p], y[:p]), (X1[p:], X2[p:], y[p:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "88ec0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(8, 20),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(20, 10),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(10, 4),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(4, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "1e23a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myLoss(m1_predict, m2_predict, total_m):\n",
    "    return (total_m - (m1_predict + m2_predict)).abs().sum() * (1/len(m1_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b26c64",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c83c2c",
   "metadata": {},
   "source": [
    "Используется оптимизатор Adam, т.к. он показал более высокую сходимость, чем SGD. Также юзается маленький learning rate + большое количество эпох, потому что было проведено исследование и выяснилось, что так модель хоть и сходится значительно дольше, но показывает более высокую точность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "2ee31b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = myLoss\n",
    "\n",
    "EPOCHS = 250\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "7bc03f63",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 | train loss: 29.30383795273455 | valid loss: 29.18927586392431\n",
      "epoch 2 | train loss: 29.012825218937063 | valid loss: 28.766313969555043\n",
      "epoch 3 | train loss: 28.402615382701537 | valid loss: 27.92302808878879\n",
      "epoch 4 | train loss: 27.31265552436249 | valid loss: 26.513051714391516\n",
      "epoch 5 | train loss: 25.914523308790184 | valid loss: 24.910410879843408\n",
      "epoch 6 | train loss: 24.45567812346205 | valid loss: 23.064435283808308\n",
      "epoch 7 | train loss: 23.04919303670714 | valid loss: 21.229367285526916\n",
      "epoch 8 | train loss: 21.972986254511 | valid loss: 19.70104769902658\n",
      "epoch 9 | train loss: 21.291970061350472 | valid loss: 18.634746298258808\n",
      "epoch 10 | train loss: 20.93018175906773 | valid loss: 17.96240274265937\n",
      "epoch 11 | train loss: 20.76128158690054 | valid loss: 17.560592120541937\n",
      "epoch 12 | train loss: 20.6833959840521 | valid loss: 17.326775203934485\n",
      "epoch 13 | train loss: 20.639580580252634 | valid loss: 17.18278265335846\n",
      "epoch 14 | train loss: 20.605324486388437 | valid loss: 17.08469789613864\n",
      "epoch 15 | train loss: 20.57192350414735 | valid loss: 17.00436956771637\n",
      "epoch 16 | train loss: 20.536671879925304 | valid loss: 16.9299060648127\n",
      "epoch 17 | train loss: 20.499140387094474 | valid loss: 16.854515500620597\n",
      "epoch 18 | train loss: 20.45898638420467 | valid loss: 16.773646913443113\n",
      "epoch 19 | train loss: 20.416295394112794 | valid loss: 16.68733733005664\n",
      "epoch 20 | train loss: 20.37377573266814 | valid loss: 16.60777896580345\n",
      "epoch 21 | train loss: 20.33144475840315 | valid loss: 16.523161341452845\n",
      "epoch 22 | train loss: 20.287847870512852 | valid loss: 16.44873426096913\n",
      "epoch 23 | train loss: 20.245876863032958 | valid loss: 16.3718469122631\n",
      "epoch 24 | train loss: 20.204437859450714 | valid loss: 16.29321090404213\n",
      "epoch 25 | train loss: 20.16327277919914 | valid loss: 16.2146921357563\n",
      "epoch 26 | train loss: 20.122600971143456 | valid loss: 16.13405307543228\n",
      "epoch 27 | train loss: 20.082281834717037 | valid loss: 16.0546067303284\n",
      "epoch 28 | train loss: 20.042887502833256 | valid loss: 15.977321462338198\n",
      "epoch 29 | train loss: 20.005351130720936 | valid loss: 15.914294414427678\n",
      "epoch 30 | train loss: 19.97026328648193 | valid loss: 15.851421797436016\n",
      "epoch 31 | train loss: 19.93700805193261 | valid loss: 15.791782919794914\n",
      "epoch 32 | train loss: 19.905370953716808 | valid loss: 15.735499464868395\n",
      "epoch 33 | train loss: 19.875245549256288 | valid loss: 15.684007902507613\n",
      "epoch 34 | train loss: 19.846425092672998 | valid loss: 15.636200261320049\n",
      "epoch 35 | train loss: 19.81867841301085 | valid loss: 15.59316685704064\n",
      "epoch 36 | train loss: 19.791869381560556 | valid loss: 15.553851268876453\n",
      "epoch 37 | train loss: 19.76590541872797 | valid loss: 15.518117866376281\n",
      "epoch 38 | train loss: 19.74068413731418 | valid loss: 15.488351213904\n",
      "epoch 39 | train loss: 19.715968405144125 | valid loss: 15.462595374783566\n",
      "epoch 40 | train loss: 19.691477881956704 | valid loss: 15.442128503870833\n",
      "epoch 41 | train loss: 19.66728851765017 | valid loss: 15.425162521907412\n",
      "epoch 42 | train loss: 19.643395463877088 | valid loss: 15.410639858231532\n",
      "epoch 43 | train loss: 19.619628296622746 | valid loss: 15.400200786613484\n",
      "epoch 44 | train loss: 19.595733844026736 | valid loss: 15.39262800717064\n",
      "epoch 45 | train loss: 19.571457644052142 | valid loss: 15.387617538672444\n",
      "epoch 46 | train loss: 19.546591303771056 | valid loss: 15.385168269536866\n",
      "epoch 47 | train loss: 19.52086308560794 | valid loss: 15.387006920832315\n",
      "epoch 48 | train loss: 19.49410572761222 | valid loss: 15.392491113779835\n",
      "epoch 49 | train loss: 19.466222039506405 | valid loss: 15.399636735955749\n",
      "epoch 50 | train loss: 19.437244989449464 | valid loss: 15.408727838692052\n",
      "epoch 51 | train loss: 19.40698919341534 | valid loss: 15.419779397271713\n",
      "epoch 52 | train loss: 19.375203755837454 | valid loss: 15.43295926263381\n",
      "epoch 53 | train loss: 19.341736704488344 | valid loss: 15.449704789509259\n",
      "epoch 54 | train loss: 19.30641243955757 | valid loss: 15.468325485823087\n",
      "epoch 55 | train loss: 19.269102172006534 | valid loss: 15.491249630999196\n",
      "epoch 56 | train loss: 19.22949166841145 | valid loss: 15.517950139090814\n",
      "epoch 57 | train loss: 19.1875690395319 | valid loss: 15.548099955776065\n",
      "epoch 58 | train loss: 19.143068055563337 | valid loss: 15.580912048260812\n",
      "epoch 59 | train loss: 19.096445402767085 | valid loss: 15.617248748459925\n",
      "epoch 60 | train loss: 19.047612136677852 | valid loss: 15.658378284352025\n",
      "epoch 61 | train loss: 18.99629826711703 | valid loss: 15.705117738850033\n",
      "epoch 62 | train loss: 18.942841855031027 | valid loss: 15.755049745259697\n",
      "epoch 63 | train loss: 18.88729379146914 | valid loss: 15.812545193367365\n",
      "epoch 64 | train loss: 18.829823762555666 | valid loss: 15.874789179060448\n",
      "epoch 65 | train loss: 18.770804976360708 | valid loss: 15.940915241402763\n",
      "epoch 66 | train loss: 18.710368307330942 | valid loss: 16.012171237585523\n",
      "epoch 67 | train loss: 18.648945348172248 | valid loss: 16.08809630314712\n",
      "epoch 68 | train loss: 18.58720382859435 | valid loss: 16.168565980773522\n",
      "epoch 69 | train loss: 18.525582844697976 | valid loss: 16.252356042615205\n",
      "epoch 70 | train loss: 18.464820874642722 | valid loss: 16.341205820655258\n",
      "epoch 71 | train loss: 18.40577537274059 | valid loss: 16.43350170976209\n",
      "epoch 72 | train loss: 18.349135162709633 | valid loss: 16.529706485110502\n",
      "epoch 73 | train loss: 18.295494698270968 | valid loss: 16.628449721480255\n",
      "epoch 74 | train loss: 18.245518183406396 | valid loss: 16.72778710382142\n",
      "epoch 75 | train loss: 18.199589989607848 | valid loss: 16.825194266407614\n",
      "epoch 76 | train loss: 18.158009175258346 | valid loss: 16.921046997508995\n",
      "epoch 77 | train loss: 18.120809996429877 | valid loss: 17.01398588700355\n",
      "epoch 78 | train loss: 18.08790942234329 | valid loss: 17.105660643060006\n",
      "epoch 79 | train loss: 18.059323142600967 | valid loss: 17.193863313322055\n",
      "epoch 80 | train loss: 18.03519343575345 | valid loss: 17.276879828631124\n",
      "epoch 81 | train loss: 18.01537734270096 | valid loss: 17.354122033439193\n",
      "epoch 82 | train loss: 17.99926838467393 | valid loss: 17.425933739053637\n",
      "epoch 83 | train loss: 17.98632817177833 | valid loss: 17.490492705342433\n",
      "epoch 84 | train loss: 17.975953283943706 | valid loss: 17.548728122967937\n",
      "epoch 85 | train loss: 17.96772513283959 | valid loss: 17.600150955300226\n",
      "epoch 86 | train loss: 17.961201236217835 | valid loss: 17.645449763738387\n",
      "epoch 87 | train loss: 17.956022536452814 | valid loss: 17.685339566543703\n",
      "epoch 88 | train loss: 17.951897903333737 | valid loss: 17.719601212527152\n",
      "epoch 89 | train loss: 17.948600215247914 | valid loss: 17.749322965542056\n",
      "epoch 90 | train loss: 17.945927503742748 | valid loss: 17.774460696711145\n",
      "epoch 91 | train loss: 17.94373626045034 | valid loss: 17.7959567297509\n",
      "epoch 92 | train loss: 17.941897054261798 | valid loss: 17.814196185486303\n",
      "epoch 93 | train loss: 17.94033107501042 | valid loss: 17.829403795267986\n",
      "epoch 94 | train loss: 17.938978595069692 | valid loss: 17.842349350992304\n",
      "epoch 95 | train loss: 17.937797606745853 | valid loss: 17.853069338540575\n",
      "epoch 96 | train loss: 17.93674724313277 | valid loss: 17.862217681267023\n",
      "epoch 97 | train loss: 17.935817954660973 | valid loss: 17.870186412190673\n",
      "epoch 98 | train loss: 17.934969391249403 | valid loss: 17.876948210448944\n",
      "epoch 99 | train loss: 17.934191375593596 | valid loss: 17.88276869970441\n",
      "epoch 100 | train loss: 17.933460820324814 | valid loss: 17.887624425715778\n",
      "epoch 101 | train loss: 17.93277197170861 | valid loss: 17.891914036087474\n",
      "epoch 102 | train loss: 17.932120611396016 | valid loss: 17.89558900696543\n",
      "epoch 103 | train loss: 17.931501106370852 | valid loss: 17.898904960530004\n",
      "epoch 104 | train loss: 17.93092051035241 | valid loss: 17.901373261512475\n",
      "epoch 105 | train loss: 17.930362756493725 | valid loss: 17.90375010205933\n",
      "epoch 106 | train loss: 17.929825347435624 | valid loss: 17.90603149604769\n",
      "epoch 107 | train loss: 17.92930603781833 | valid loss: 17.908103663206806\n",
      "epoch 108 | train loss: 17.92880268715605 | valid loss: 17.90970959019352\n",
      "epoch 109 | train loss: 17.92831035656265 | valid loss: 17.911218965995516\n",
      "epoch 110 | train loss: 17.927829986886135 | valid loss: 17.912818874568543\n",
      "epoch 111 | train loss: 17.927356911610953 | valid loss: 17.914217239873924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 112 | train loss: 17.92689517932602 | valid loss: 17.915375668299912\n",
      "epoch 113 | train loss: 17.926438657543326 | valid loss: 17.91644825749478\n",
      "epoch 114 | train loss: 17.92599342593664 | valid loss: 17.917427792024405\n",
      "epoch 115 | train loss: 17.925553755669654 | valid loss: 17.918538381392036\n",
      "epoch 116 | train loss: 17.92512566153007 | valid loss: 17.9196452224946\n",
      "epoch 117 | train loss: 17.924704498882534 | valid loss: 17.920719318726825\n",
      "epoch 118 | train loss: 17.92429012663757 | valid loss: 17.92169176376672\n",
      "epoch 119 | train loss: 17.923881964592994 | valid loss: 17.922538674761213\n",
      "epoch 120 | train loss: 17.92347670733174 | valid loss: 17.923282791757728\n",
      "epoch 121 | train loss: 17.923081124130682 | valid loss: 17.92402415177023\n",
      "epoch 122 | train loss: 17.922692028782034 | valid loss: 17.924816337665245\n",
      "epoch 123 | train loss: 17.922313482701025 | valid loss: 17.92569781325693\n",
      "epoch 124 | train loss: 17.92194363135326 | valid loss: 17.926712477486966\n",
      "epoch 125 | train loss: 17.921583060976825 | valid loss: 17.927673797901402\n",
      "epoch 126 | train loss: 17.92122945755343 | valid loss: 17.9285599269446\n",
      "epoch 127 | train loss: 17.920888047429578 | valid loss: 17.929451603219654\n",
      "epoch 128 | train loss: 17.92055350843864 | valid loss: 17.930283735817703\n",
      "epoch 129 | train loss: 17.920221100125133 | valid loss: 17.931106489114992\n",
      "epoch 130 | train loss: 17.919896320451663 | valid loss: 17.932082565590907\n",
      "epoch 131 | train loss: 17.91957790640336 | valid loss: 17.9329609921737\n",
      "epoch 132 | train loss: 17.91926522647278 | valid loss: 17.933922250164777\n",
      "epoch 133 | train loss: 17.91895829952216 | valid loss: 17.93474558536423\n",
      "epoch 134 | train loss: 17.918652751023256 | valid loss: 17.93564625303708\n",
      "epoch 135 | train loss: 17.918351681172094 | valid loss: 17.93641600484504\n",
      "epoch 136 | train loss: 17.918054708951637 | valid loss: 17.937204343494827\n",
      "epoch 137 | train loss: 17.917765441574627 | valid loss: 17.937961546441596\n",
      "epoch 138 | train loss: 17.91748032313359 | valid loss: 17.938722990311188\n",
      "epoch 139 | train loss: 17.91720285596727 | valid loss: 17.93960134378038\n",
      "epoch 140 | train loss: 17.916928255105322 | valid loss: 17.940453710328384\n",
      "epoch 141 | train loss: 17.916657053217104 | valid loss: 17.94120521723899\n",
      "epoch 142 | train loss: 17.91638974147507 | valid loss: 17.941972729107796\n",
      "epoch 143 | train loss: 17.916126252729683 | valid loss: 17.942775438827287\n",
      "epoch 144 | train loss: 17.915866801255866 | valid loss: 17.9434517813786\n",
      "epoch 145 | train loss: 17.915610070470013 | valid loss: 17.944042422049264\n",
      "epoch 146 | train loss: 17.915358688257918 | valid loss: 17.944697118536855\n",
      "epoch 147 | train loss: 17.915110244781157 | valid loss: 17.945427501726\n",
      "epoch 148 | train loss: 17.914867884750606 | valid loss: 17.946100839986066\n",
      "epoch 149 | train loss: 17.91462868376623 | valid loss: 17.946664798679112\n",
      "epoch 150 | train loss: 17.9143911245503 | valid loss: 17.947342767435313\n",
      "epoch 151 | train loss: 17.914158394065083 | valid loss: 17.947885831435723\n",
      "epoch 152 | train loss: 17.91392740569537 | valid loss: 17.94838411292711\n",
      "epoch 153 | train loss: 17.9137000803706 | valid loss: 17.948835323100656\n",
      "epoch 154 | train loss: 17.913475948798506 | valid loss: 17.949266386468782\n",
      "epoch 155 | train loss: 17.91325786898408 | valid loss: 17.949802597596445\n",
      "epoch 156 | train loss: 17.913041906266272 | valid loss: 17.950222917083433\n",
      "epoch 157 | train loss: 17.912831959845146 | valid loss: 17.95068764382342\n",
      "epoch 158 | train loss: 17.91262493254263 | valid loss: 17.951191447109622\n",
      "epoch 159 | train loss: 17.912419600577294 | valid loss: 17.95171501855679\n",
      "epoch 160 | train loss: 17.91221335794352 | valid loss: 17.95215529710187\n",
      "epoch 161 | train loss: 17.912011605274827 | valid loss: 17.952658900757402\n",
      "epoch 162 | train loss: 17.911810994902744 | valid loss: 17.953056625660576\n",
      "epoch 163 | train loss: 17.9116132530985 | valid loss: 17.95349375239866\n",
      "epoch 164 | train loss: 17.91141802977912 | valid loss: 17.953894426853324\n",
      "epoch 165 | train loss: 17.911225276657298 | valid loss: 17.95425357717428\n",
      "epoch 166 | train loss: 17.911037400553496 | valid loss: 17.954745149586177\n",
      "epoch 167 | train loss: 17.910849545575395 | valid loss: 17.95520319836291\n",
      "epoch 168 | train loss: 17.910666292981258 | valid loss: 17.95559759592918\n",
      "epoch 169 | train loss: 17.91048456370076 | valid loss: 17.956001230625574\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [468]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(valid_dataset[\u001b[38;5;241m0\u001b[39m])):\n\u001b[0;32m     16\u001b[0m     x1, x2, y \u001b[38;5;241m=\u001b[39m valid_dataset[\u001b[38;5;241m0\u001b[39m][i], valid_dataset[\u001b[38;5;241m1\u001b[39m][i], valid_dataset[\u001b[38;5;241m2\u001b[39m][i]\n\u001b[1;32m---> 17\u001b[0m     o1, o2 \u001b[38;5;241m=\u001b[39m model(x1), \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(o1, o2, y)\n\u001b[0;32m     19\u001b[0m     valid_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py:98\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# можно не обучать самостоятельно, уже обученная модель импортируется ниже\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for i in range(0, len(train_dataset[0]), BATCH_SIZE):\n",
    "        step = min(len(train_dataset[0]), i+BATCH_SIZE)\n",
    "        x1, x2, y = train_dataset[0][i:step], train_dataset[1][i:step], train_dataset[2][i:step]\n",
    "        optimizer.zero_grad()\n",
    "        o1, o2 = model(x1), model(x2)\n",
    "        loss = criterion(o1, o2, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item()/BATCH_SIZE)\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    for i in range(len(valid_dataset[0])):\n",
    "        x1, x2, y = valid_dataset[0][i], valid_dataset[1][i], valid_dataset[2][i]\n",
    "        o1, o2 = model(x1), model(x2)\n",
    "        loss = criterion(o1, o2, y)\n",
    "        valid_loss.append(loss.item())\n",
    "    print(f\"epoch {e+1} | train loss: {np.array(train_loss).mean()} | valid loss: {np.array(valid_loss).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1652bace",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65000ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_prediction = torch.load(\"mass_prediction_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b270d64",
   "metadata": {},
   "source": [
    "Также добавим вполне стандартных физических признаков для нашей задачи: разница между энергиями, вектор импульса, кинетическая энергия, ускорение и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbc6fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    m1 = mass_prediction(torch.FloatTensor(df[['E1', 'px1', 'py1', 'pz1', 'pt1', 'eta1', 'phi1', 'Q1']].values)).detach().numpy().flatten()\n",
    "    m2 = df[\"M\"] - m1\n",
    "    df[\"M1\"] = m1\n",
    "    df[\"M2\"] = m2\n",
    "    df['pz_diff'] = df.pz1 - df.pz2\n",
    "    df['pt1^2'] = df.pt1**2\n",
    "    df['pt2^2'] = df.pt2**2\n",
    "    df['eta_diff'] = df.eta1 - df.eta2\n",
    "    df[\"Total_E\"] = df.E1 + df.E2\n",
    "    df[\"MomentumVector_E1\"] = df.px1 + df.py1 +df.pz1\n",
    "    df[\"MomentumVector_E2\"] = df.px2 + df.py2 + df.pz2\n",
    "    df[\"KineticEnergy_E1\"] = (df.px1*df.px1) + (df.py1*df.py1) + (df.pz1*df.pz1)\n",
    "    df[\"KineticEnergy_E2\"] = (df.px2*df.px2) + (df.py2*df.py2) + (df.pz2*df.pz2)\n",
    "    df[\"TotalMomentumVector\"] = (df.px1 + df.px2) + (df.py1 + df.py2) + (df.pz1 + df.pz2)\n",
    "    df[\"TotalKineticEnergy\"] = (df.px1*df.px1) + (df.py1*df.py1) + (df.pz1*df.pz1) + (df.px2*df.px2) + (df.py2*df.py2) + (df.pz2*df.pz2)\n",
    "    df[\"EnergyMomentum_E1\"] = df.E1/df.pt1\n",
    "    df[\"EnergyMomentum_E2\"] = df.E2/df.pt2\n",
    "    df[\"TotalTransverseMomentum\"] = df.pt1 + df.pt2\n",
    "    df[\"TotalPseudorapidity\"] = df.eta1 + df.eta2\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e53f7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feature_engineering(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "229c0327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>px1</th>\n",
       "      <th>py1</th>\n",
       "      <th>pz1</th>\n",
       "      <th>pt1</th>\n",
       "      <th>eta1</th>\n",
       "      <th>phi1</th>\n",
       "      <th>Q1</th>\n",
       "      <th>E2</th>\n",
       "      <th>px2</th>\n",
       "      <th>py2</th>\n",
       "      <th>pz2</th>\n",
       "      <th>pt2</th>\n",
       "      <th>eta2</th>\n",
       "      <th>phi2</th>\n",
       "      <th>M</th>\n",
       "      <th>Q2</th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>pz_diff</th>\n",
       "      <th>pt1^2</th>\n",
       "      <th>pt2^2</th>\n",
       "      <th>eta_diff</th>\n",
       "      <th>Total_E</th>\n",
       "      <th>MomentumVector_E1</th>\n",
       "      <th>MomentumVector_E2</th>\n",
       "      <th>KineticEnergy_E1</th>\n",
       "      <th>KineticEnergy_E2</th>\n",
       "      <th>TotalMomentumVector</th>\n",
       "      <th>TotalKineticEnergy</th>\n",
       "      <th>EnergyMomentum_E1</th>\n",
       "      <th>EnergyMomentum_E2</th>\n",
       "      <th>TotalTransverseMomentum</th>\n",
       "      <th>TotalPseudorapidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.08500</td>\n",
       "      <td>-4.591070</td>\n",
       "      <td>20.110600</td>\n",
       "      <td>12.43260</td>\n",
       "      <td>20.62800</td>\n",
       "      <td>0.571144</td>\n",
       "      <td>1.795240</td>\n",
       "      <td>-1</td>\n",
       "      <td>2.57188</td>\n",
       "      <td>-2.15672</td>\n",
       "      <td>-1.30836</td>\n",
       "      <td>0.501343</td>\n",
       "      <td>2.52255</td>\n",
       "      <td>0.197459</td>\n",
       "      <td>-2.59630</td>\n",
       "      <td>12.0101</td>\n",
       "      <td>1</td>\n",
       "      <td>10.439666</td>\n",
       "      <td>1.570434</td>\n",
       "      <td>11.931257</td>\n",
       "      <td>425.514384</td>\n",
       "      <td>6.363259</td>\n",
       "      <td>0.373685</td>\n",
       "      <td>26.65688</td>\n",
       "      <td>27.952130</td>\n",
       "      <td>-2.963737</td>\n",
       "      <td>580.083699</td>\n",
       "      <td>6.614592</td>\n",
       "      <td>24.988393</td>\n",
       "      <td>586.698291</td>\n",
       "      <td>1.167588</td>\n",
       "      <td>1.019556</td>\n",
       "      <td>23.15055</td>\n",
       "      <td>0.768603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.94630</td>\n",
       "      <td>-4.677580</td>\n",
       "      <td>5.187980</td>\n",
       "      <td>-21.85720</td>\n",
       "      <td>6.98534</td>\n",
       "      <td>-1.858470</td>\n",
       "      <td>2.304500</td>\n",
       "      <td>1</td>\n",
       "      <td>67.16120</td>\n",
       "      <td>-8.24769</td>\n",
       "      <td>11.37090</td>\n",
       "      <td>-65.675800</td>\n",
       "      <td>14.04710</td>\n",
       "      <td>-2.246710</td>\n",
       "      <td>2.19832</td>\n",
       "      <td>4.0102</td>\n",
       "      <td>1</td>\n",
       "      <td>11.212400</td>\n",
       "      <td>-7.202200</td>\n",
       "      <td>43.818600</td>\n",
       "      <td>48.794975</td>\n",
       "      <td>197.321018</td>\n",
       "      <td>0.388240</td>\n",
       "      <td>90.10750</td>\n",
       "      <td>-21.346800</td>\n",
       "      <td>-62.552590</td>\n",
       "      <td>526.532083</td>\n",
       "      <td>4510.632463</td>\n",
       "      <td>-83.899390</td>\n",
       "      <td>5037.164546</td>\n",
       "      <td>3.284922</td>\n",
       "      <td>4.781143</td>\n",
       "      <td>21.03244</td>\n",
       "      <td>-4.105180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.39342</td>\n",
       "      <td>0.346488</td>\n",
       "      <td>-1.935140</td>\n",
       "      <td>1.36514</td>\n",
       "      <td>1.96592</td>\n",
       "      <td>0.648077</td>\n",
       "      <td>-1.393620</td>\n",
       "      <td>1</td>\n",
       "      <td>33.86190</td>\n",
       "      <td>-30.87370</td>\n",
       "      <td>-8.12102</td>\n",
       "      <td>11.291100</td>\n",
       "      <td>31.92390</td>\n",
       "      <td>0.346700</td>\n",
       "      <td>-2.88438</td>\n",
       "      <td>11.0103</td>\n",
       "      <td>1</td>\n",
       "      <td>10.755643</td>\n",
       "      <td>0.254657</td>\n",
       "      <td>-9.925960</td>\n",
       "      <td>3.864841</td>\n",
       "      <td>1019.135391</td>\n",
       "      <td>0.301377</td>\n",
       "      <td>36.25532</td>\n",
       "      <td>-0.223512</td>\n",
       "      <td>-27.703620</td>\n",
       "      <td>5.728428</td>\n",
       "      <td>1146.625257</td>\n",
       "      <td>-27.927132</td>\n",
       "      <td>1152.353685</td>\n",
       "      <td>1.217455</td>\n",
       "      <td>1.060707</td>\n",
       "      <td>33.88982</td>\n",
       "      <td>0.994777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80.32260</td>\n",
       "      <td>-14.435900</td>\n",
       "      <td>26.110100</td>\n",
       "      <td>74.57610</td>\n",
       "      <td>29.83510</td>\n",
       "      <td>1.647090</td>\n",
       "      <td>2.075850</td>\n",
       "      <td>1</td>\n",
       "      <td>22.38180</td>\n",
       "      <td>2.30876</td>\n",
       "      <td>7.14518</td>\n",
       "      <td>21.084600</td>\n",
       "      <td>7.50892</td>\n",
       "      <td>1.755900</td>\n",
       "      <td>1.25826</td>\n",
       "      <td>12.0104</td>\n",
       "      <td>-1</td>\n",
       "      <td>10.895534</td>\n",
       "      <td>1.114866</td>\n",
       "      <td>53.491500</td>\n",
       "      <td>890.133192</td>\n",
       "      <td>56.383880</td>\n",
       "      <td>-0.108810</td>\n",
       "      <td>102.70440</td>\n",
       "      <td>86.250300</td>\n",
       "      <td>30.538540</td>\n",
       "      <td>6451.727222</td>\n",
       "      <td>500.944327</td>\n",
       "      <td>116.788840</td>\n",
       "      <td>6952.671549</td>\n",
       "      <td>2.692218</td>\n",
       "      <td>2.980695</td>\n",
       "      <td>37.34402</td>\n",
       "      <td>3.402990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.20864</td>\n",
       "      <td>2.846250</td>\n",
       "      <td>-0.351847</td>\n",
       "      <td>-1.43891</td>\n",
       "      <td>2.86791</td>\n",
       "      <td>-0.482756</td>\n",
       "      <td>-0.122994</td>\n",
       "      <td>1</td>\n",
       "      <td>27.96590</td>\n",
       "      <td>-4.05881</td>\n",
       "      <td>-9.86542</td>\n",
       "      <td>-25.851300</td>\n",
       "      <td>10.66770</td>\n",
       "      <td>-1.618370</td>\n",
       "      <td>-1.96111</td>\n",
       "      <td>11.0105</td>\n",
       "      <td>1</td>\n",
       "      <td>9.692578</td>\n",
       "      <td>1.317922</td>\n",
       "      <td>24.412390</td>\n",
       "      <td>8.224908</td>\n",
       "      <td>113.799823</td>\n",
       "      <td>1.135614</td>\n",
       "      <td>31.17454</td>\n",
       "      <td>1.055493</td>\n",
       "      <td>-39.775530</td>\n",
       "      <td>10.295397</td>\n",
       "      <td>782.090162</td>\n",
       "      <td>-38.720037</td>\n",
       "      <td>792.385559</td>\n",
       "      <td>1.118808</td>\n",
       "      <td>2.621549</td>\n",
       "      <td>13.53561</td>\n",
       "      <td>-2.101126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         E1        px1        py1       pz1       pt1      eta1      phi1  Q1  \\\n",
       "0  24.08500  -4.591070  20.110600  12.43260  20.62800  0.571144  1.795240  -1   \n",
       "1  22.94630  -4.677580   5.187980 -21.85720   6.98534 -1.858470  2.304500   1   \n",
       "2   2.39342   0.346488  -1.935140   1.36514   1.96592  0.648077 -1.393620   1   \n",
       "3  80.32260 -14.435900  26.110100  74.57610  29.83510  1.647090  2.075850   1   \n",
       "4   3.20864   2.846250  -0.351847  -1.43891   2.86791 -0.482756 -0.122994   1   \n",
       "\n",
       "         E2       px2       py2        pz2       pt2      eta2     phi2  \\\n",
       "0   2.57188  -2.15672  -1.30836   0.501343   2.52255  0.197459 -2.59630   \n",
       "1  67.16120  -8.24769  11.37090 -65.675800  14.04710 -2.246710  2.19832   \n",
       "2  33.86190 -30.87370  -8.12102  11.291100  31.92390  0.346700 -2.88438   \n",
       "3  22.38180   2.30876   7.14518  21.084600   7.50892  1.755900  1.25826   \n",
       "4  27.96590  -4.05881  -9.86542 -25.851300  10.66770 -1.618370 -1.96111   \n",
       "\n",
       "         M  Q2         M1        M2    pz_diff       pt1^2        pt2^2  \\\n",
       "0  12.0101   1  10.439666  1.570434  11.931257  425.514384     6.363259   \n",
       "1   4.0102   1  11.212400 -7.202200  43.818600   48.794975   197.321018   \n",
       "2  11.0103   1  10.755643  0.254657  -9.925960    3.864841  1019.135391   \n",
       "3  12.0104  -1  10.895534  1.114866  53.491500  890.133192    56.383880   \n",
       "4  11.0105   1   9.692578  1.317922  24.412390    8.224908   113.799823   \n",
       "\n",
       "   eta_diff    Total_E  MomentumVector_E1  MomentumVector_E2  \\\n",
       "0  0.373685   26.65688          27.952130          -2.963737   \n",
       "1  0.388240   90.10750         -21.346800         -62.552590   \n",
       "2  0.301377   36.25532          -0.223512         -27.703620   \n",
       "3 -0.108810  102.70440          86.250300          30.538540   \n",
       "4  1.135614   31.17454           1.055493         -39.775530   \n",
       "\n",
       "   KineticEnergy_E1  KineticEnergy_E2  TotalMomentumVector  \\\n",
       "0        580.083699          6.614592            24.988393   \n",
       "1        526.532083       4510.632463           -83.899390   \n",
       "2          5.728428       1146.625257           -27.927132   \n",
       "3       6451.727222        500.944327           116.788840   \n",
       "4         10.295397        782.090162           -38.720037   \n",
       "\n",
       "   TotalKineticEnergy  EnergyMomentum_E1  EnergyMomentum_E2  \\\n",
       "0          586.698291           1.167588           1.019556   \n",
       "1         5037.164546           3.284922           4.781143   \n",
       "2         1152.353685           1.217455           1.060707   \n",
       "3         6952.671549           2.692218           2.980695   \n",
       "4          792.385559           1.118808           2.621549   \n",
       "\n",
       "   TotalTransverseMomentum  TotalPseudorapidity  \n",
       "0                 23.15055             0.768603  \n",
       "1                 21.03244            -4.105180  \n",
       "2                 33.88982             0.994777  \n",
       "3                 37.34402             3.402990  \n",
       "4                 13.53561            -2.101126  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3c29f",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c883a",
   "metadata": {},
   "source": [
    "Попробуем использовать алгоритм снижения размерности Umap, чтобы визуализировать распределение данных и посмотреть, образуют ли они какие-то кластеры, которые могут стать важным признаком для нашей задачи. Я посчитал, что для этой задачи будет лучше интерпретировать данные о обеих частицах из эксперимента по-отдельности (это действительно показало лучшие результаты, чем если брать их вместе)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55681503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DjGle\\AppData\\Local\\Temp\\ipykernel_7488\\2508672455.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q1.rename(columns={n: n.replace(\"1\", \"\") for n in df_q1.columns}, inplace=True)\n",
      "C:\\Users\\DjGle\\AppData\\Local\\Temp\\ipykernel_7488\\2508672455.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q2.rename(columns={n: n.replace(\"2\", \"\") for n in df_q2.columns}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_q1 = df[['E1', 'px1', 'py1', 'pz1', 'pt1', 'eta1', 'phi1', \"M1\", 'MomentumVector_E1', \"KineticEnergy_E1\", \"EnergyMomentum_E1\"]]\n",
    "df_q2 = df[['E2', 'px2', 'py2', 'pz2', 'pt2', 'eta2', 'phi2', \"M2\", 'MomentumVector_E2', \"KineticEnergy_E2\", \"EnergyMomentum_E2\"]]\n",
    "df_q1.rename(columns={n: n.replace(\"1\", \"\") for n in df_q1.columns}, inplace=True)\n",
    "df_q2.rename(columns={n: n.replace(\"2\", \"\") for n in df_q2.columns}, inplace=True)\n",
    "\n",
    "df_every_q = pd.concat([df_q1, df_q2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed9a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучение модели занимает оооочень много времени, поэтому можно импортировать уже обученную мной\n",
    "\n",
    "umap_model = umap.UMAP(n_neighbors=20)\n",
    "embedding = umap_model.fit_transform(df_every_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d3fbdc",
   "metadata": {},
   "source": [
    "Также попробуем кластеризировать данные с помощью к-средних (10 кластеров показали себя наиболее хорошо)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d432b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans.fit(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ecfc56",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "703346c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = pickle.load(open(\"umap_clusterizator.pkl\", \"rb\"))\n",
    "kmeans = pickle.load(open(\"kmeans_clusterizator.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa947a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = umap_model.transform(df_every_q)\n",
    "labels = kmeans.predict(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe19d3a",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7515e3de",
   "metadata": {},
   "source": [
    "Визуализируем кластеризованные нами данные (при самостоятельном обучении, график распределения может быть другим, т.к. в umap есть некоторая доля рандомности, но тем не менее, общий тренд будет схож)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f76b4d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"r\", \"g\", \"b\", \"yellow\", \"pink\", \"orange\", \"c\", \"m\", 'aquamarine', 'mediumseagreen']\n",
    "labels_color = [colors[l] for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b51c049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1f1cd4a3730>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(embedding[:, 0], embedding[:, 1], color=labels_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb29ed34",
   "metadata": {},
   "source": [
    "Результат просто невероятен! График получился очень интересным и красивым. В данных определённо скрыта какая-то красивая закономерность, если при снижении их размерности мы можем наблюдать такое. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b41a61d",
   "metadata": {},
   "source": [
    "Создадим 4 дополнительных признака: эмбеддинги для каждой частицы и к какому кластеру относится каждая частица."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd5e3698",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_embedds_q1 = embedding[:len(embedding)//2]\n",
    "cluster_group_q1 = labels[:len(embedding)//2]\n",
    "cluster_embedds_q2 = embedding[len(embedding)//2:]\n",
    "cluster_group_q2 = labels[len(embedding)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf0aa31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cluster_embedds_q1\"] = cluster_embedds_q1.tolist()\n",
    "df[\"cluster_group_q1\"] = cluster_group_q1\n",
    "df[\"cluster_embedds_q2\"] = cluster_embedds_q2.tolist()\n",
    "df[\"cluster_group_q2\"] = cluster_group_q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef7941",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccba866",
   "metadata": {},
   "source": [
    "В подобных задачах часто помогает генерирование эмбеддингов с помощью нейронных сетей, т.к. бОльшая часть наших признакми некатегориальные, а числовые, и нейронная сеть может вывести корреляции, которые не сможет обнаружить модель, основанная, например, на деревьях решений (т.к. нейронная сеть может по-другому трансформировать пространство признаков). Поэтому мы можем обучить нейронную сеть для задачи классификации целевой переменной, и хоть она будет показывать невысокую точность, мы можем убрать у неё последний слой, и использовать для генерации эмбеддингов, с которыми умеют работать некоторые модели, по-типу CatBoosting'а."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15c726c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DjGle\\AppData\\Local\\Temp\\ipykernel_7488\\2346949502.py:1: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  X = torch.FloatTensor(df[set(df.columns) - set([\"Q2\", \"cluster_embedds_q1\", \"cluster_embedds_q2\"])].values)\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor(df[set(df.columns) - set([\"Q2\", \"cluster_embedds_q1\", \"cluster_embedds_q2\"])].values)\n",
    "y = torch.LongTensor((df[\"Q2\"]>0).astype(int).values) #1 => 1, -1 => 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "57383ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(35, 128),\n",
    "                      nn.Dropout(0.25),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 2),\n",
    "                      nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "32037fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "7b0e1691",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for e in range(EPOCHS):\n",
    "    model.train()\n",
    "    for i in range(0, len(train_dataset[0]), BATCH_SIZE):\n",
    "        step = min(len(train_dataset[0]), i+BATCH_SIZE)\n",
    "        x, l = X[i:step], y[i:step]\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x), l)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item()/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "318a3c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"embedding_generator.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ced2e523",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"embeddings\"] = model[0](X).sigmoid().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56348743",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7910b48",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c60a562",
   "metadata": {},
   "source": [
    "#### Выбор и обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a807f97b",
   "metadata": {},
   "source": [
    "На выбор и обучение моделей ушла бОльшая часть времени. Были опробованы почти все классические модели машинного обучения для задач классификации (нейронки совсем печально работали, как полносвязки, так рекурренты).  \n",
    "И в ходе валидации моделей было замечено одно крайне важное наблюдение: разные модели выдают правильные предикты на разных сэмплах валидационной выборки, хоть их суммарная точность и оставалась примерно одинаковой. Поэтому я решил попытаться обучить те модели, у которых объединение множеств правильных ответов будет максимальным. А затем подобрать алгоритм, который на основании вероятностных предсказаний каждой модели будет выносить общий предикт, к какому классу относится сэмпл."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc36bb2",
   "metadata": {},
   "source": [
    "В свой своеобразный ансамбль моделей я включил следующие:  \n",
    "\n",
    "1. CatBoost - во-первых, потому что он единственный умел работать с фичами эмбеддингами;\n",
    "2. Random Forest - общая точность чуть ниже, чем у catboost'а, но на некоторых сэмплах, на которых catboost работал плохо, он выдавал правильные предикты. Предположительно, потому что юзал другие фичи;\n",
    "3. К-ближайших;\n",
    "4. LGBM;\n",
    "5. Логистическая регрессия.  \n",
    "\n",
    "Пробовал просто деревья решений, метод опорных векторов, разные более классические методы градиентного бустинга. Но в купе с остальными алгоритмами \"площадь покрытия\" правильных ответов была меньше.  \n",
    "\n",
    "Для используемых в ансамбле моделей были тчательно подобраны гипперпараметры. Ещё стоит отметить, что разным моделям на вход подавались разные фичи, с которыми они показывали бОльшую точность. А также для разных моделей данные по-разному предобрабатывались, как было написано ещё в пункте с анализом и выбором выбросов. Например, для того-же SVC данные нормализировались перед обучением (хоть он и не вошёл в конечном итоге в ансамбль).  \n",
    "\n",
    "Модели обучались на разных частях выборки, а для оценки точности первоначально использовалась кросс-валидация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9ab975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_training(model_idx, train, val):\n",
    "    if model_idx == 1:\n",
    "        print(\"Training CatBoost\")\n",
    "        clf = CatBoostClassifier(iterations=1000, learning_rate=0.05)\n",
    "        train_pool = Pool(train.drop(\"Q2\", axis=1), train[\"Q2\"], cat_features=[\"cluster_group_q1\", \"cluster_group_q2\"],\n",
    "                                    embedding_features=[\"cluster_embedds_q1\", \"cluster_embedds_q2\", \"embeddings\"])\n",
    "        valid_pool = Pool(val.drop(\"Q2\", axis=1), val[\"Q2\"], cat_features=[\"cluster_group_q1\", \"cluster_group_q2\"],\n",
    "                                    embedding_features=[\"cluster_embedds_q1\", \"cluster_embedds_q2\", \"embeddings\"])\n",
    "        clf.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=50, verbose=50)\n",
    "        return clf, clf.predict_proba(valid_pool)[:, 1], clf.predict_proba(train_pool)[:, 1]\n",
    "    if model_idx == 2:\n",
    "        print(\"Training Random Forest\")\n",
    "        X_train, y_train = train.drop([\"Q2\", \"cluster_embedds_q1\", \"cluster_embedds_q2\", \"embeddings\"], axis=1), train[\"Q2\"]\n",
    "        X_valid, y_valid = val.drop([\"Q2\", \"cluster_embedds_q1\", \"cluster_embedds_q2\", \"embeddings\"], axis=1), val[\"Q2\"]\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X_train, y_train)\n",
    "        return clf, clf.predict_proba(X_valid)[:, 1], clf.predict_proba(X_train)[:, 1]\n",
    "    if model_idx == 3:\n",
    "        print(\"Training KNN\")\n",
    "        X_train, y_train = train.drop([\"Q2\", \"cluster_embedds_q1\", \"cluster_embedds_q2\", \"embeddings\"], axis=1), train[\"Q2\"]\n",
    "        X_valid, y_valid = val.drop([\"Q2\", \"cluster_embedds_q1\", \"cluster_embedds_q2\", \"embeddings\"], axis=1), val[\"Q2\"]\n",
    "        clf = KNeighborsClassifier(n_neighbors=5)\n",
    "        clf.fit(X_train, y_train)\n",
    "        return clf, clf.predict_proba(X_valid)[:, 1], clf.predict_proba(X_train)[:, 1]\n",
    "    if model_idx == 4:\n",
    "        print(\"Training LGBM\")\n",
    "        X_train, y_train = train.drop([\"Q2\", \"cluster_embedds_q1\", \"cluster_embedds_q2\", \"embeddings\"], axis=1), train[\"Q2\"]\n",
    "        X_valid, y_valid = val.drop([\"Q2\", \"cluster_embedds_q1\", \"cluster_embedds_q2\", \"embeddings\"], axis=1), val[\"Q2\"]\n",
    "        clf = lgb.LGBMClassifier(objective=\"binary\", metric=\"auc\", learning_rate=0.05, max_depth=5, num_leaves=64, n_estimators=2000)\n",
    "        clf.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=50, verbose=50)\n",
    "        return clf, clf.predict_proba(X_valid)[:, 1], clf.predict_proba(X_train)[:, 1]\n",
    "    if model_idx == 5:\n",
    "        print(\"Training Logistic regression\")\n",
    "        X_train, y_train = train.drop([\"Q2\", \"cluster_embedds_q1\", \"cluster_embedds_q2\", \"embeddings\"], axis=1), train[\"Q2\"]\n",
    "        X_valid, y_valid = val.drop([\"Q2\", \"cluster_embedds_q1\", \"cluster_embedds_q2\", \"embeddings\"], axis=1), val[\"Q2\"]\n",
    "        clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "        return clf, clf.predict_proba(X_valid)[:, 1], clf.predict_proba(X_train)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4266919f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CatBoost\n",
      "0:\tlearn: 0.6878866\ttest: 0.6878503\tbest: 0.6878503 (0)\ttotal: 268ms\tremaining: 4m 27s\n",
      "50:\tlearn: 0.6417598\ttest: 0.6427586\tbest: 0.6427586 (50)\ttotal: 4.14s\tremaining: 1m 16s\n",
      "100:\tlearn: 0.6387827\ttest: 0.6417503\tbest: 0.6417043 (93)\ttotal: 7.88s\tremaining: 1m 10s\n",
      "150:\tlearn: 0.6365102\ttest: 0.6412828\tbest: 0.6412600 (141)\ttotal: 11.8s\tremaining: 1m 6s\n",
      "200:\tlearn: 0.6345980\ttest: 0.6412277\tbest: 0.6411584 (184)\ttotal: 15.8s\tremaining: 1m 2s\n",
      "250:\tlearn: 0.6324973\ttest: 0.6411618\tbest: 0.6411123 (226)\ttotal: 19.6s\tremaining: 58.4s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.6411123487\n",
      "bestIteration = 226\n",
      "\n",
      "Shrink model to first 227 iterations.\n",
      "Training with fold 1 completed\n",
      "Training Random Forest\n",
      "Training with fold 2 completed\n",
      "Training KNN\n",
      "Training with fold 3 completed\n",
      "Training LGBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DjGle\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\DjGle\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's auc: 0.630498\n",
      "Training with fold 4 completed\n",
      "Training Logistic regression\n",
      "Training with fold 5 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DjGle\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "targets = df[\"Q2\"].values\n",
    "train_data_target = df\n",
    "feature_cols = list(train_data_target.columns).remove(\"Q2\")\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=100, shuffle=True)\n",
    "oof = np.zeros(len(train_data_target))\n",
    "train_preds = np.zeros(len(train_data_target))\n",
    "\n",
    "models = []\n",
    "\n",
    "\n",
    "for fold_, (train_idx, val_idx) in enumerate(cv.split(train_data_target, targets), 1):\n",
    "    train, val = train_data_target.iloc[train_idx], train_data_target.iloc[val_idx]\n",
    "    clf, valid_predicts, train_predicts = models_training(fold_, train, val)\n",
    "    oof[val_idx] = valid_predicts\n",
    "    train_preds[train_idx] += train_predicts / (cv.n_splits-1)\n",
    "    models.append(clf)\n",
    "    print(f\"Training with fold {fold_} completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd659feb",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b80a9814",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [pickle.load(open(f\"classifiers_ensemble//{model}.pkl\", \"rb\")) for model in [\"catboost\", \n",
    "                                                                                      \"random_forest\",\n",
    "                                                                                      \"knn\",\n",
    "                                                                                      \"lgb\",\n",
    "                                                                                      \"lreg\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13fe228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensember_booster = pickle.load(open(\"model_ensembler.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b27db",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c28f2",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150c0890",
   "metadata": {},
   "source": [
    "Оценим точность работы нашей модели. Конечно, оценка будет не совсем корректной, т.к. мы валидируем на том же, на чём и обучали. Но точность на тестовой выборке отдельно мы можем всегда оценить, отправив сабмит на каггл :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b42ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_targets = df[\"Q2\"].values\n",
    "df.drop('Q2', axis=1, inplace=True)\n",
    "pool_for_eval = Pool(df, cat_features=[\"cluster_group_q1\", \"cluster_group_q2\"],\n",
    "                                    embedding_features=[\"cluster_embedds_q1\", \"cluster_embedds_q2\", \"embeddings\"])\n",
    "df_for_eval = df.drop([\"cluster_embedds_q1\", \"cluster_embedds_q2\", \"embeddings\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56d4fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_probs = []\n",
    "for i in range(len(models)):\n",
    "    if i == 0:\n",
    "        predicts = models[i].predict_proba(pool_for_eval)[:, 1]\n",
    "    else:\n",
    "        predicts = models[i].predict_proba(df_for_eval)[:, 1]\n",
    "    total_probs.append(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0a5339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_probs = np.array(total_probs).transpose((1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "079a100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = ensember_booster.predict(total_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7acf786a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9474651121709945"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicts == eval_targets).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d84c9",
   "metadata": {},
   "source": [
    "В принципе, явного переобучения нет, точность модели не 99-100%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45691c0",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3696d29",
   "metadata": {},
   "source": [
    "#### Осуществим предсказания для тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5092606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DjGle\\AppData\\Local\\Temp\\ipykernel_22096\\2963318818.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q1.rename(columns={n: n.replace(\"1\", \"\") for n in df_q1.columns}, inplace=True)\n",
      "C:\\Users\\DjGle\\AppData\\Local\\Temp\\ipykernel_22096\\2963318818.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q2.rename(columns={n: n.replace(\"2\", \"\") for n in df_q2.columns}, inplace=True)\n",
      "C:\\Users\\DjGle\\AppData\\Local\\Temp\\ipykernel_22096\\2963318818.py:24: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  X = torch.FloatTensor(test_df[set(test_df.columns) - set([\"Q2\", \"cluster_embedds_q1\", \"cluster_embedds_q2\"])].values)\n"
     ]
    }
   ],
   "source": [
    "test_df = feature_engineering(test_df)\n",
    "test_df[\"M\"] = test_df[\"M\"].fillna(test_df[\"M\"].mean())\n",
    "test_df[\"M2\"] = test_df[\"M2\"].fillna(test_df[\"M2\"].mean())\n",
    "\n",
    "df_q1 = test_df[['E1', 'px1', 'py1', 'pz1', 'pt1', 'eta1', 'phi1', \"M1\", 'MomentumVector_E1', \"KineticEnergy_E1\", \"EnergyMomentum_E1\"]]\n",
    "df_q2 = test_df[['E2', 'px2', 'py2', 'pz2', 'pt2', 'eta2', 'phi2', \"M2\", 'MomentumVector_E2', \"KineticEnergy_E2\", \"EnergyMomentum_E2\"]]\n",
    "df_q1.rename(columns={n: n.replace(\"1\", \"\") for n in df_q1.columns}, inplace=True)\n",
    "df_q2.rename(columns={n: n.replace(\"2\", \"\") for n in df_q2.columns}, inplace=True)\n",
    "\n",
    "df_every_q = pd.concat([df_q1, df_q2], axis=0)\n",
    "embedding = umap_model.transform(df_every_q)\n",
    "labels = kmeans.predict(embedding)\n",
    "\n",
    "cluster_embedds_q1 = embedding[:len(embedding)//2]\n",
    "cluster_group_q1 = labels[:len(embedding)//2]\n",
    "cluster_embedds_q2 = embedding[len(embedding)//2:]\n",
    "cluster_group_q2 = labels[len(embedding)//2:]\n",
    "\n",
    "test_df[\"cluster_embedds_q1\"] = cluster_embedds_q1.tolist()\n",
    "test_df[\"cluster_group_q1\"] = cluster_group_q1\n",
    "test_df[\"cluster_embedds_q2\"] = cluster_embedds_q2.tolist()\n",
    "test_df[\"cluster_group_q2\"] = cluster_group_q2\n",
    "\n",
    "X = torch.FloatTensor(test_df[set(test_df.columns) - set([\"Q2\", \"cluster_embedds_q1\", \"cluster_embedds_q2\"])].values)\n",
    "test_df[\"embeddings\"] = model[0](X).sigmoid().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d81b5473",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pool = Pool(test_df, cat_features=[\"cluster_group_q1\", \"cluster_group_q2\"],\n",
    "                                    embedding_features=[\"cluster_embedds_q1\", \"cluster_embedds_q2\", \"embeddings\"])\n",
    "test_df = test_df.drop([\"cluster_embedds_q1\", \"cluster_embedds_q2\", \"embeddings\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4802249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_probs = []\n",
    "for i in range(len(models)):\n",
    "    if i == 0:\n",
    "        predicts = models[i].predict_proba(test_pool)[:, 1]\n",
    "    else:\n",
    "        predicts = models[i].predict_proba(test_df)[:, 1]\n",
    "    total_probs.append(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a5f4a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_probs = np.array(total_probs).transpose((1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a95abeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = ensember_booster.predict(total_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d21ae0",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9ab568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({\"Event\": events, \"Q2\": predicts})\n",
    "sub.to_csv(\"sub10_ensemble_plus_knn.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
